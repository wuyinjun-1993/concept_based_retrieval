{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "query_path = \"/home/wuyinjun/concept_based_retrieval/\"\n",
    "\n",
    "img_caption_file_name= os.path.join(query_path, \"prod_hard_negatives/prod_vg_hard_negs_swap_all.csv\")\n",
    "\n",
    "caption_pd = pd.read_csv(img_caption_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyinjun/miniconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wuyinjun/miniconda3/envs/myenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# processor = ViTImageProcessor.from_pretrained('google/vit-large-patch16-224')\n",
    "# model = ViTForImageClassification.from_pretrained('google/vit-large-patch16-224').to(device)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "# processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "raw_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "# processor =  lambda images: raw_processor(images=images, return_tensors=\"pt\", padding=False, do_resize=False, do_center_crop=False)[\"pixel_values\"]\n",
    "processor =  lambda images: raw_processor(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "text_processor =  lambda text: raw_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "img_processor =  lambda images: raw_processor(images=images, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_ls = list(caption_pd[\"caption\"])\n",
    "id_ls = list(caption_pd[\"image_id\"])\n",
    "\n",
    "\n",
    "text_feat_ls = []\n",
    "visited_id_ls = []\n",
    "visited_caption_ls = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for idx in range(len(caption_ls)):\n",
    "        if id_ls[idx] in visited_id_ls:\n",
    "            continue\n",
    "        caption = caption_ls[idx]\n",
    "        inputs = text_processor(caption)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        # print(idx)\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "        text_feat_ls.append(text_features.cpu().view(-1))\n",
    "        visited_id_ls.append(id_ls[idx])\n",
    "        visited_caption_ls.append(caption)\n",
    "    \n",
    "text_feat_tensor = torch.stack(text_feat_ls, dim=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wuyinjun/miniconda3/envs/myenv/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "print(len(text_feat_tensor))\n",
    "clustering = KMeans(n_clusters=500).fit(text_feat_tensor.detach().numpy())\n",
    "# clustering = Birch(threshold=0.9, n_clusters=None).fit(text_feat_tensor.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([138, 158,  37, ..., 236, 247, 193], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  6,  8,  4, 37,  8, 13, 11,  6, 15, 22,  8,  8, 14, 19, 15, 16,  6,\n",
      "        17,  2,  2, 12, 14, 15,  4, 17,  3, 10,  6,  8,  8,  7, 23,  4,  8, 17,\n",
      "        23,  4,  7, 17,  9, 16, 14,  7, 17,  4, 10, 14, 11,  7,  9,  4, 13, 10,\n",
      "         1, 10, 19,  3, 10,  7,  3,  2,  5, 15, 13,  8, 14,  9,  7,  3, 10, 10,\n",
      "         5, 35, 14, 18,  3,  4,  5,  5, 12,  5,  3,  9, 15,  7, 10,  6, 10, 30,\n",
      "        13, 15, 20, 22, 12,  9, 18, 25,  6,  4, 22, 14, 13,  9, 14, 16,  8,  6,\n",
      "         5,  9, 14,  6,  8, 14,  4,  6, 11,  7,  9, 12,  7,  5, 12,  5, 11,  9,\n",
      "        11, 10,  8,  8,  8,  7,  8,  6,  4, 13,  9, 30, 16,  9,  9,  1,  9, 28,\n",
      "         8,  7,  9, 12, 17, 25, 16,  9, 16, 16, 18, 14, 11, 13, 30,  1, 12,  5,\n",
      "        10,  7,  4,  4,  2,  9,  7, 14, 37,  7,  1,  4, 22,  5,  8,  8,  5, 20,\n",
      "         4,  4, 14, 14,  4, 16,  4,  8,  6, 18,  8, 10,  7,  8,  5,  6,  4, 12,\n",
      "        14,  8, 13, 10, 16,  6,  9,  4, 22, 13,  6,  5,  7, 10,  4,  7,  7,  8,\n",
      "         3, 14,  5,  7,  1,  6, 20,  9,  5,  5, 15, 11,  3,  5,  6, 10, 17, 14,\n",
      "        14,  7, 12, 16, 14,  9, 11, 15, 10,  4,  7, 14,  3,  8,  5, 14, 10,  7,\n",
      "        17,  5,  3, 10,  7,  4, 10, 11,  7,  7,  8, 10, 10,  6,  8, 10,  6,  1,\n",
      "         8,  1, 12,  5,  3, 15,  7,  6,  9, 10,  8,  3,  5, 24, 11, 10, 15,  3,\n",
      "         5, 18, 41,  9,  5,  7,  8,  6,  5,  6,  8, 18, 18,  6,  3,  7, 16, 13,\n",
      "         8,  5,  4, 11, 10,  4, 10,  6, 12,  2,  9,  9,  3, 12,  4,  2,  6,  9,\n",
      "         2,  5, 11, 13,  6,  4, 25, 13, 14,  6, 11, 10, 20,  7,  3,  3,  1, 15,\n",
      "         8,  5,  3,  3,  8,  4, 12,  3,  7,  6, 10,  2,  9, 12,  8,  4,  4,  4,\n",
      "         8,  1,  6, 11, 15, 11,  5, 11,  7, 19,  5,  8,  9,  3,  3,  7,  6, 12,\n",
      "        15, 13,  2,  5,  3,  2,  9,  6,  8,  6,  6, 10, 10,  8,  6, 14,  5, 28,\n",
      "        11,  1,  9,  4,  4,  4,  9,  9,  2,  3, 11,  1,  5,  5,  7, 11,  5,  1,\n",
      "         4,  7,  1,  7,  5,  4,  5,  3,  1, 13,  3,  4, 15,  7,  2, 10,  5, 10,\n",
      "         9,  8,  5, 11,  5, 11,  9,  8,  2,  2,  6,  3,  8, 12, 10, 12,  2,  4,\n",
      "         4,  3,  7, 10, 11,  6,  1,  2,  9,  3,  7, 13,  7,  4,  7,  4,  2, 10,\n",
      "         7, 10,  8,  4, 12,  9,  1,  8,  2,  3,  5,  6,  4,  2, 10,  7,  1,  3,\n",
      "         4,  5,  4,  3,  4,  7,  4,  5,  2, 10,  7, 13,  7,  7])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(clustering.labels_)\n",
    "import numpy as np\n",
    "unique_label_ids=np.unique(np.array(clustering.labels_))\n",
    "label_count_ls = [0 for _ in range(len(unique_label_ids))]\n",
    "\n",
    "for label in clustering.labels_:\n",
    "    label_count_ls[label] += 1\n",
    "\n",
    "label_count_tensor = torch.tensor(label_count_ls)\n",
    "\n",
    "sorted_label_ids = torch.argsort(label_count_tensor, descending=False)\n",
    "\n",
    "\n",
    "# print(torch.nonzero(label_count_tensor.view(-1) > 1).view(-1))\n",
    "\n",
    "print(label_count_tensor)\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2359340 a dog behind a boot and a shoe. the shoe has a shoelace. the dog has a head and nose.\n",
      "2366126 a clock with a design under the hand and a number on it, on the side of a building with a statue inside\n",
      "2360386 a dude with a tattoo on his arm holding a cell phone in his arm next to a box with a blue lighter.\n",
      "2369881 photo of pants, another pants, a frisbee, and a brown shoe. there is a girl wearing a green coat.\n",
      "2361534 there is a crack in the wall and a crack in the sidewalk. a woman is standing next to another woman. the first woman has hair.\n",
      "2367935 bare trees on trail. trail has tracks. sign on tree. snow has tracks. snow pants are for snow.\n",
      "2359509 a green light on a speaker that has a light. there is a cat laying on a desk. the cat has eyes.\n",
      "2371219 pen, blue flowers, and almonds on a plate with a pie\n",
      "2360526 photo of statues, another statues, and another statues, with an angel next to a circle\n",
      "2365780 person wearing blue jeans with a folder holding papers and legs on a bed\n",
      "2363138 tab on another can. hole of can. ground near can. line down another can. word on back of can.\n",
      "1017 a black artwork with a white mat in the background\n",
      "2359418 mannequin wearing a jacket with a graphic print. there is a glass panel in front of the mannequin, and lights are reflected off the glass.\n",
      "1020 girl with a ponytail in an art room with several paintings.\n",
      "989 astronaut with a flag on their spacesuit, wearing gloves\n",
      "2361692 a row of seahorses in the sky, with grass around sand and a seahorse in the foreground.\n",
      "447 stad selling food, bags on step, child of step, and power lies over step\n"
     ]
    }
   ],
   "source": [
    "label_id_ls = torch.nonzero(label_count_tensor.view(-1) == 1).view(-1).tolist()\n",
    "label_array = torch.tensor(clustering.labels_)\n",
    "for label_id in label_id_ls:\n",
    "    sample_ids = torch.nonzero(label_array == label_id).view(-1).tolist()\n",
    "    for sample_id in sample_ids:\n",
    "        print(visited_id_ls[sample_id], visited_caption_ls[sample_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "[1, 5, 6, 7, 8, 9, 11, 13, 16, 23, 27, 30, 31, 34, 37, 38, 39, 40, 43, 47, 48, 53, 58, 60, 62, 64, 69, 72, 74, 79, 82, 97, 98, 130, 156, 166, 171, 179, 187, 194, 206, 221, 236, 239, 243, 246, 252, 265, 272, 277, 286, 291, 303, 313, 314, 320, 356, 368, 369, 375, 397, 414, 442, 444, 453, 462, 479, 492, 494, 502, 508, 524, 534, 538, 545, 556, 559, 560, 568, 578, 586, 597, 608, 621, 626, 630, 675, 681, 696, 697, 701, 714, 736, 738, 748, 759, 833, 870, 879, 889, 913, 917, 930, 934, 939, 940, 951, 955, 956, 964, 968, 986, 989, 991, 994, 997, 1001, 1015, 1017, 1020, 1027, 1034, 1040, 1042, 1043, 1053, 1057, 1066, 1090, 1091, 1114, 1115, 1129, 1145, 1150, 1159, 1170, 1179, 1183, 1184, 1185, 1196, 1199, 1216, 1233, 1241, 1269, 1270, 1285, 1314, 1321, 1326, 1327, 1339, 1345, 1346, 1351, 1373, 1378, 1392, 1466, 1473, 1501, 1528, 1532, 1549, 1891, 1925, 1928, 2066, 2114, 2275, 2480, 2643, 2704, 3114, 3173, 3367, 3371, 3433, 3548, 3587, 3616, 3636, 3764, 3766, 3784, 3851, 3914, 3996, 4126, 4167, 4181, 4214, 4245, 4276, 4294, 4419, 4759, 4921, 4969, 2364447, 2364468, 2364574, 2364629, 2364704, 2364718, 2364814, 2364824, 2364847, 2364865, 2364918, 2364954, 2365016, 2365032, 2365088, 2365114, 2365134, 2365140, 2365144, 2365186, 2365188, 2365255, 2365551, 2365577, 2365588, 2365630, 2365709, 2365889, 2366058, 2366063, 2366105, 2366212, 2366265, 2366444, 2366445, 2366474, 2366544, 2366545, 2366650, 2366769, 2366783, 2367122, 2367132, 2367405, 2367471, 2368137, 2368566, 2368807, 2368974, 2369990, 2359298, 2359300, 2359310, 2359326, 2359382, 2359394, 2359425, 2359501, 2359505, 2359509, 2359521, 2359527, 2359528, 2359546, 2359550, 2359558, 2359569, 2359573, 2359600, 2359606, 2359616, 2359617, 2359627, 2359639, 2359645, 2359653, 2359670, 2359703, 2359735, 2359768, 2359777, 2359782, 2359790, 2359837, 2359840, 2359867, 2359885, 2359893, 2359911, 2359954, 2359963, 2359967, 2359971, 2359974, 2360008, 2360012, 2360033, 2360052, 2360057, 2360072, 2360096, 2360136, 2360138, 2360141, 2360163, 2360169, 2360172, 2360173, 2360190, 2360201, 2360226, 2360320, 2360332, 2360366, 2360378, 2360379, 2360382, 2360389, 2360393, 2360417, 2360472, 2360494, 2360531, 2360533, 2360542, 2360578, 2360629, 2360729, 2360740, 2360763, 2360793, 2360833, 2360857, 2360870, 2360874, 2359393, 2359418, 2359423, 2359451, 2359462, 2359494, 2359557, 2359602, 2359692, 2359738, 2359741, 2359769, 2359774, 2359779, 2359786, 2359798, 2359915, 2360092, 2360145, 2360154, 2360279, 2360357, 2360390, 2360506, 2360509, 2360526, 2360530, 2360544, 2360558, 2360603, 2360639, 2360676, 2360719, 2360725, 2360742, 2360757, 2360759, 2360808, 2360832, 2360891, 2360909, 2360981, 2361045, 2361055, 2361072, 2361084, 2361110, 2361128, 2361238, 2361249, 2361272, 2361301, 2361350, 2361381, 2361463, 2361517, 2361662, 2361764, 2359314, 2359340, 2359347, 2359351, 2359374, 2359409, 2359541, 2359634, 2359745, 2359848, 2359886, 2359895, 2359905, 2359917, 2359946, 2360146, 2360152, 2360157, 2360250, 2360265, 2360288, 2360386, 2360401, 2360402, 2360411, 2360551, 2360568, 2360582, 2360718, 2360800, 2360822, 2360886, 2361065, 2361070, 2361104, 2361237, 2361268, 2361345, 2361362, 2361373, 2361421, 2361477, 2361534, 2361692, 2361706, 2361720, 2361730, 2361821, 2361912, 2361994, 2362026, 2362038, 2362055, 2362151, 2362179, 2362217, 2362248, 2362276, 2362289, 2362346, 2362422, 2362602, 2362662, 2362669, 2362734, 2362824, 2362907, 2362963, 2362988, 2363091, 2363138, 2363198, 2363223, 2363328, 2363506, 2363508, 2363538, 2363712, 2361871, 2363490, 2363860, 2363928, 2363952, 2364068, 2364105, 2364193, 2364196, 2364256, 2364712, 2365185, 2365287, 2365291, 2365704, 2365780, 2366037, 2366126, 2366957, 2367648, 2367935, 2368185, 2369524, 2369764, 2369881, 2371219, 2371708, 447]\n"
     ]
    }
   ],
   "source": [
    "visited_labels = set()\n",
    "\n",
    "selected_img_id_ls = []\n",
    "\n",
    "for idx in range(len(clustering.labels_)):\n",
    "    label = clustering.labels_[idx]\n",
    "    if label in visited_labels:\n",
    "        continue\n",
    "    \n",
    "    selected_img_id_ls.append(visited_id_ls[idx])\n",
    "    \n",
    "    visited_labels.add(label)\n",
    "\n",
    "print(len(selected_img_id_ls))\n",
    "print(selected_img_id_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"prod_hard_negatives/selected_img_id_ls\", \"wb\") as f:\n",
    "    pickle.dump(selected_img_id_ls, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
