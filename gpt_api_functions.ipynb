{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bFcLULTvQ-p",
        "outputId": "9fa403b3-9c2f-4e79-b409-e0a7741bf668"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.33.0-py3-none-any.whl (325 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/325.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/325.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m317.4/325.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.33.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "img_caption_file_name = '/content/drive/MyDrive/mscoco_40k_clustered.pkl'\n",
        "\n",
        "with open(img_caption_file_name, 'rb') as f:\n",
        "  df_mscoco = pickle.load(f)"
      ],
      "metadata": {
        "id": "Ii_5DYYzHuN6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The openai secret key can be created by setting up an account on https://platform.openai.com/docs/overview, then load in a minimum credit of $5, and creating a project api key on https://platform.openai.com/api-keys. Information on rate limits and costs are here https://platform.openai.com/docs/guides/rate-limits and https://openai.com/api/pricing/."
      ],
      "metadata": {
        "id": "nUWynBWcLkU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a batch size of 25, it is recommended to load \\$50 to enter tier 1 user status. For a bartch size of 35, it is recommended to load \\$100 to enter tier 2 user status."
      ],
      "metadata": {
        "id": "c65p5bf6Mkxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One run of 500 queries rakes roughly 1000 seconds and costs \\$25."
      ],
      "metadata": {
        "id": "AjbkaWPKMvAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decomposing"
      ],
      "metadata": {
        "id": "p2oW5kxxJU8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import ssl\n",
        "import certifi\n",
        "import time\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Settings\n",
        "MODEL = \"gpt-4o\"\n",
        "OPENAI_SECRET_KEY = \"sk-proj-X...X\"\n",
        "MAX_RETRIES = 3  # Maximum number of retries\n",
        "BATCH_SIZE = 25\n",
        "\n",
        "# Asynchronous function to decompose a sentence\n",
        "async def decompose_sentence_async(session, sentence, retries=MAX_RETRIES):\n",
        "    payload = {\n",
        "        'model': MODEL,\n",
        "        'messages': [\n",
        "            {\"role\": \"user\", \"content\": f\"Example 1:\\nOriginal: An institutional looking room holds an exit sign, chairs, recording equipment and a man with one arm aloft that appears to be demonstrating something with a device in the held-up hand for the benefit of a woman behind him that is watching closely.\\n\\nDecomposed form: \\\"An exit sign.\\\"|\\\"Chairs.\\\"|\\\"Recording equipment.\\\"|\\\"A man with one arm aloft appears to be demonstrating with a device in the held-up hand.\\\"|\\\"A woman behind a man is watching him closely.\\\"\\n\\nExample 2:\\nOriginal: Black night, a blurry shot shows a tall fence that angles, in a kind of Z shape, around what appears to be a parking area with a large building fronting it, as a kid with a red helmet takes a fierce swing in the batting area, inside the fence.\\n\\nDecomposed form: \\\"In a black night, a blurry shot shows a tall fence that angles, in a kind of Z shape.\\\"|\\\"Around appears to be a parking area with a large building fronting it.\\\"|\\\"A kid with a red helmet takes a fierce swing in the batting area, inside the fence.\\\"\\n\\nExample 3:\\nOriginal: An older looking model computer components; two disk drive boxes stacked, a keyboard unit, and a more modern looking computer monitor with apple logo center bottom, and on screen at top centered in light blue font letters is printed \\\"APPLE II\\\"\\n\\nDecomposed form: \\\"An older looking model computer components.\\\"|\\\"Computer disk drive boxes stacked.\\\"|\\\"A keyboard unit.\\\"|\\\"A modern computer monitor with apple logo at the center bottom.\\\"|\\\"On the screen of a modern computer monitor, at the top center, light blue font letters print the words APPLE II.\\\"\\n\\nExample 4:\\nOriginal: A motel-like room features all of its furniture in rows, including a sofa, double bed, stuffed chair, desk chair, desk, bureau and television, all of which, like the one piece of artwork, carpet, linen and window drapes, are all in neutrals.\\n\\nDecomposed form: \\\"A motel room.\\\"|\\\"A neutral colored sofa.\\\"|\\\"A neutral colored double bed.\\\"|\\\"A neutral colored stuffed chair.\\\"|\\\"A desk chair.\\\"|\\\"A desk.\\\"|\\\"A bureau.\\\"|\\\"A television.\\\"|\\\"One piece of artwork.\\\"|\\\"Neutral colored linen.\\\"|\\\"Neutral colored window drapes.\\\"\\n\\nExample 5:\\nOriginal: A room shows a twin bed, crib, double bed, and a round bed, all featuring checks and pillows, either white, or checked, high windows, a brick wall, a piece of art against a white wall, and a separate vertical surface with framed items on it.\\n\\nDecomposed form: \\\"A room.\\\"|\\\"A twin bed featuring checks and pillows, either white, or checked.\\\"|\\\"A crib.\\\"|\\\"A double bed featuring checks and pillows, either white, or checked.\\\"|\\\"A round bed featuring checks and pillows, either white, or checked.\\\"|\\\"High windows.\\\"|\\\"A brick wall.\\\"|\\\"A piece of art against a white wall.\\\"|\\\"A vertical surface with framed items on it.\\\"\\n\\nExample 6:\\nOriginal: A street view shows a building entrance and traffic, including a bus, in the background, and to the foreground, a black iron fence, and a bench with an older, seated person, looking at a flock of pigeons scattered on the ground and bench.\\n\\nDecomposed form: \\\"A street view shows a building entrance.\\\"|\\\"A street view shows traffic, including a bus in the background.\\\"|\\\"In the foreground is a black iron fence.\\\"|\\\"In the foreground is a bench with an older person sitting on it\\\"|\\\"A flock of pigeons scattered on the ground and bench.\\\"\\n\\nExample 7:\\nOriginal: Off in the distance, a few people dot the stands and the fenced in area bordering a ball field, while an umpire and three uniformed baseball players, including a batter, catcher and outfielder, strike playing poses in the foreground.\\n\\nDecomposed form: \\\"A few people dot the stands bordering a ball field.\\\"|\\\"A few people dot the fenced-in area bordering a ball field.\\\"|\\\"A baseball umpire.\\\"|\\\"A baseball batter.\\\"|\\\"A baseball catcher.\\\"|\\\"An outfielder.\\\"\\n\\nExample 8:\\nOriginal: A counter top with a plate with a fork and few scraps of food and a teddy bear lying on side with arm outstretched on plate near fork, with another plate with an apple and two bowls with produce, a canister and some metal objects.\\n\\nDecomposed form: \\\"A counter top.\\\"|\\\"A plate with a fork and few scraps of food.\\\"|\\\"A teddy bear lying with arm outstretched on plate near fork.\\\"|\\\"A plate with an apple.\\\"|\\\"Bowls with apples.\\\"|\\\"A canister.\\\"|\\\"Metal objects.\\\"\\n\\nExample 9:\\nOriginal: Indirect lighting in a darkened room plays across a table covered with the remains of a meal, thereby giving everything, including the table, a basket, plates with uneaten food, utensils and glasses with liquid, a reddish patina.\\n\\nDecomposed form: \\\"Indirect lighting in a darkened room shines across a table covered with the remains of a meal.\\\"|\\\"The table has a reddish patina.\\\"|\\\"The basket has a reddish patina.\\\"|\\\"The plates with uneaten food have a reddish patina.\\\"|\\\"Utensils have a reddish patina.\\\"|\\\"Glasses with liquid have a reddish patina.\\\"\\n\\nExample 10:\\nOriginal: the long view of a parking lot shows a row of parked vehicles, including a bus, which appears to be the destination of a number of similarly dressed women with luggage that are moving towards it from the immediate foreground.\\n\\nDecomposed form: \\\"A parking lot shows a row of parked vehicles.\\\"|\\\"A bus.\\\"|\\\"Similarly dressed women with luggages.\\\"\\n\\nExample 11:\\nOriginal: Corner view of a table with plaid cloth, red and white checkerboard napkins, a pizza, a container of green leafy vegetable, and some bottles, with the hips of a person wearing an apron standing beside table in green grass.\\n\\nDecomposed form: \\\"Corner view of a table with plaid cloth.\\\"|\\\"Red and white checkerboard napkins.\\\"|\\\"A pizza.\\\"|\\\"A container of green leafy vegetables.\\\"|\\\"Some bottles.\\\"|\\\"The hips of a person wearing an apron standing beside a table in green grass.\\\"\\n\\nExample 12:\\nOriginal: A room with natural, unfinished, pale wood walls and built in shelves, also has curtained sliding doors that open onto a patio with a lounge chair, a big double bed with a blue blanket, and various framed photos and books.\\n\\nDecomposed form: \\\"A room.\\\"|\\\"Natural, unfinished, pale wood walls.\\\"|\\\"Built-in shelves.\\\"|\\\"Curtained sliding doors.\\\"|\\\"Sliding doors that open onto a patio with a lounge chair.\\\"|\\\"A big double bed with a blue blanket.\\\"|\\\"Framed photos.\\\"|\\\"Books.\\\"\\n\\nExample 13:\\nOriginal: A minimally furnished room shows small window pairs with blinds and no curtains, an unmade double bed with a heavy, taupe duvet, and a white border near the ceiling with colorful birds trailing in both directions.\\n\\nDecomposed form: \\\"A minimally furnished room.\\\"|\\\"Small window pairs with blinds and no curtains.\\\"|\\\"An unmade double bed with a heavy, taupe duvet.\\\"|\\\"A white border near the ceiling with colorful birds trailing in both directions.\\\"\\n\\nExample 14:\\nOriginal: A blurry night-time street scene shows a higher and lower roadway with overpasses and lots of traffic going the same way, while to the left an area of red glare with an arrow and number, suggests a caution area.\\n\\nDecomposed form: \\\"A blurry night-time street scene shows a higher and lower roadway with overpasses and lots of traffic going the same way.\\\"|\\\"An area of red glare with an arrow and number, suggests a caution area.\\\"\\n\\nExample 15:\\nOriginal: Several people are standing, or sitting, in the vicinity of a scoreboard, to the front of which lies a tennis court, featuring a player with his knees bent, who is also grasping a racquet with both hands.\\n\\nDecomposed form: \\\"Several people are standing, or sitting, in the vicinity of a scoreboard.\\\"|\\\"In front of the scoreboard lies a tennis court.\\\"|\\\"A tennis player with his knees bent, who is grasping a racquet with both hands.\\\"\\n\\nExample 16:\\nOriginal: Black and white of a blacksmith interior with a horse, a man standing with arm out to it, and a man ducking under it's head and holding a long metal file in both hands, with horses on poles on the wall.\\n\\nDecomposed form: \\\"Black and white photo of a blacksmith interior with a horse.\\\"|\\\"Black and white photo of a man standing with arm out to a horse.\\\"|\\\"Black and white photo of a man ducking under a horses head and holding a long metal file in both hands.\\\"|\\\"Black and white photo with horse poles on the wall.\\\"\\n\\nExample 17:\\nOriginal: A blurry living room scene suggests a wide screen TV with a game playing, bookshelves, a low cluttered table, a couch, and closest and clearest, a sports t-shirt with the number thirty-five on it.\\n\\nDecomposed form: \\\"A blurry living room scene.\\\"|\\\"A wide screen TV with a sports game playing.\\\"|\\\"Bookshelves.\\\"|\\\"A low cluttered table.\\\"|\\\"A couch.\\\"|\\\"A sports t-shirt with the number 35 on it.\\\"\\n\\nExample 18:\\nOriginal: In the first picture a boy is flipping his skateboard, in the second picture is skating and lifting the board slightly off the ground, and in the third he is jumping into the air onto the skateboard.\\n\\nDecomposed form: \\\"A boy is flipping his skateboard.\\\"|\\\"A boy is skating and lifting the skateboard slightly off the ground.\\\"|\\\"A boy is jumping into the air onto the skateboard.\\\"\\n\\nExample 19:\\nOriginal: A table with a person holding a sandwich in hand over a plate with a knife on it, and another plate with a knife, napkin, and french bread sandwich, along with a bottle and a glass with beverages.\\n\\nDecomposed form: \\\"A table with a person holding a sandwich in hand.\\\"|\\\"A sandwich in hand over a plate with a knife on it.\\\"|\\\"A plate with a knife, napkin, and french bread sandwich.\\\"|\\\"A bottle.\\\"|\\\"A glass with beverages.\\\"\\n\\nExample 20:\\nOriginal: An impressive array of motorcycles recedes into the distance between a driveway and a building fronted with cars and a grassy area, as lots of people mill about the driveway, around the bikes.\\n\\nDecomposed form: \\\"An array of motorcycles between a driveway and a building.\\\"|\\\"A building fronted with cars and a grassy area.\\\"|\\\"Lots of people mill about the driveway, around the bikes.\\\"\\n\\nGiven these examples of decomposing original sentences into their decomposed forms, decompose the given sentence:\\n{sentence}\\nPlease response in the same format, outputing only the decomposed sentences.\"}\n",
        "        ]\n",
        "    }\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            async with session.post(\n",
        "                url='https://api.openai.com/v1/chat/completions',\n",
        "                headers={\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {OPENAI_SECRET_KEY}\"},\n",
        "                json=payload,\n",
        "                ssl=ssl.create_default_context(cafile=certifi.where())\n",
        "            ) as response:\n",
        "                response_json = await response.json()\n",
        "            if \"error\" in response_json:\n",
        "                print(f\"OpenAI request failed with error {response_json['error']}\")\n",
        "                if response_json['error']['code'] == 'rate_limit_exceeded':\n",
        "                    retry_after = 10\n",
        "                    await asyncio.sleep(retry_after)  # Wait for the retry-after duration before retrying\n",
        "                continue\n",
        "            result = response_json['choices'][0]['message']['content']\n",
        "            print(result)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"Request failed: {e}\")\n",
        "    print(\"errors\")\n",
        "    return \"\"\n",
        "\n",
        "# Asynchronous function to process multiple sentences in parallel for decomposition\n",
        "async def process_sentences_for_decomposition(sentences):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        results = []\n",
        "        for i in range(0, len(sentences), BATCH_SIZE):\n",
        "            batch = sentences[i:i+BATCH_SIZE]\n",
        "            tasks = [decompose_sentence_async(session, sentence) for sentence in batch]\n",
        "            batch_results = await asyncio.gather(*tasks)\n",
        "            results.extend(batch_results)\n",
        "            if i + BATCH_SIZE < len(sentences):\n",
        "                await asyncio.sleep(10)  # Wait for 10 seconds between batches to respect rate limits\n",
        "    return results\n",
        "\n",
        "def apply_decomposition_to_dataframe(df, sentence_col):\n",
        "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    df_copy = df.copy()\n",
        "    sentences = df_copy[sentence_col].tolist()\n",
        "    results = asyncio.run(process_sentences_for_decomposition(sentences))\n",
        "    df_copy['caption_triples_ls'] = results\n",
        "    return df_copy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    begin = time.time()\n",
        "    df = df_mscoco[0:10].copy()  # Assuming tgt_df is already defined\n",
        "    # Apply function to DataFrame\n",
        "    df = apply_decomposition_to_dataframe(df, 'caption_mscoco')\n",
        "    # Filter rows that do not start and end with \"\n",
        "    filtered_df = df[~(df['caption_triples_ls'].str.startswith('\"') & df['caption_triples_ls'].str.endswith('\"'))].copy()\n",
        "    # Apply the lambda function to those rows only\n",
        "    filtered_df['caption_triples_ls'] = filtered_df['caption_triples_ls'].apply(lambda x: '\"' + x.split('\"', 1)[-1].rsplit('\"', 1)[0] + '\"')\n",
        "    # Combine the processed rows back with the original DataFrame\n",
        "    result_df = pd.concat([df[(df['caption_triples_ls'].str.startswith('\"') & df['caption_triples_ls'].str.endswith('\"'))], filtered_df]).sort_index()\n",
        "    end = time.time()\n",
        "    total_time = end - begin\n",
        "    print(f\"Operation took a total of {total_time} seconds.\")\n",
        "    '''\n",
        "    pickle_file_path = '/content/drive/MyDrive/mscoco_500decomposed.pkl'\n",
        "    with open(pickle_file_path, 'wb') as f:\n",
        "        pickle.dump(result_df, f)\n",
        "    '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCENVeHFJWhY",
        "outputId": "e2890801-35d1-4293-90bb-7165d197c716"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"A parking lot shows a row of parked vehicles.\"|\"A bus.\"|\"Similarly dressed women with luggage moving towards a bus from the immediate foreground.\"\n",
            "\"A few people dot the stands bordering a ball field.\"|\"A few people dot the fenced-in area bordering a ball field.\"|\"A baseball umpire.\"|\"A baseball batter.\"|\"A baseball catcher.\"|\"An outfielder.\"\n",
            "\"In a black night, a blurry shot shows a tall fence that angles, in a kind of Z shape.\"|\"Around appears to be a parking area with a large building fronting it.\"|\"A kid with a red helmet takes a fierce swing in the batting area, inside the fence.\"\n",
            "\"Indirect lighting in a darkened room shines across a table covered with the remains of a meal.\"|\"The table has a reddish patina.\"|\"The basket has a reddish patina.\"|\"The plates with uneaten food have a reddish patina.\"|\"Utensils have a reddish patina.\"|\"Glasses with liquid have a reddish patina.\"\n",
            "\"An exit sign.\"|\"Chairs.\"|\"Recording equipment.\"|\"A man with one arm aloft appears to be demonstrating something with a device in the held-up hand.\"|\"A woman behind the man is watching him closely.\"\n",
            "\"An older looking model computer components.\"|\"Computer disk drive boxes stacked.\"|\"A keyboard unit.\"|\"A modern computer monitor with apple logo at the center bottom.\"|\"On the screen of a modern computer monitor, at the top center, light blue font letters print the words APPLE II.\"\n",
            "\"A street view shows a building entrance.\"|\"A street view shows traffic, including a bus in the background.\"|\"In the foreground is a black iron fence.\"|\"In the foreground is a bench with an older person sitting on it.\"|\"A flock of pigeons scattered on the ground and bench.\"\n",
            "\"A counter top.\"|\"A plate with a fork and a few scraps of food.\"|\"A teddy bear lying on its side with arm outstretched on plate near fork.\"|\"Another plate with an apple.\"|\"Two bowls with produce.\"|\"A canister.\"|\"Some metal objects.\"\n",
            "\"A motel-like room.\"|\"All of its furniture in rows.\"|\"A sofa in neutral colors.\"|\"A double bed in neutral colors.\"|\"A stuffed chair in neutral colors.\"|\"A desk chair.\"|\"A desk.\"|\"A bureau.\"|\"A television.\"|\"One piece of artwork in neutral colors.\"|\"A carpet in neutral colors.\"|\"Linen in neutral colors.\"|\"Window drapes in neutral colors.\"\n",
            "\"A room.\"|\"A twin bed featuring checks and pillows, either white, or checked.\"|\"A crib.\"|\"A double bed featuring checks and pillows, either white, or checked.\"|\"A round bed featuring checks and pillows, either white, or checked.\"|\"High windows.\"|\"A brick wall.\"|\"A piece of art against a white wall.\"|\"A vertical surface with framed items on it.\"\n",
            "Operation took a total of 3.326556444168091 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intermediate_df = result_df"
      ],
      "metadata": {
        "id": "mj5sWOUnJXI9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intermediate_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "ErU2iA1RJ2SW",
        "outputId": "3b334282-4f73-44fd-aafa-2aa564c18aa1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id             image  \\\n",
              "0  553992  000000553992.jpg   \n",
              "1   39270  000000039270.jpg   \n",
              "2  277161  000000277161.jpg   \n",
              "3  380070  000000380070.jpg   \n",
              "4  357272  000000357272.jpg   \n",
              "5  480770  000000480770.jpg   \n",
              "6  515241  000000515241.jpg   \n",
              "7  560604  000000560604.jpg   \n",
              "8  158005  000000158005.jpg   \n",
              "9  449712  000000449712.jpg   \n",
              "\n",
              "                                  caption_sharegpt4v  \\\n",
              "0  In the image, a man and a woman are standing i...   \n",
              "1  In the image, there is a man who is in the mid...   \n",
              "2  In the center of the image, a vintage Apple II...   \n",
              "3  The image captures a cozy hotel room, bathed i...   \n",
              "4  The image captures a room that exudes a vintag...   \n",
              "5  In the heart of a bustling city, an elderly wo...   \n",
              "6  The image captures a dynamic moment in a baseb...   \n",
              "7  In the center of the image, a black stuffed an...   \n",
              "8  In the dimly lit ambiance of the room, a dinne...   \n",
              "9  In the image, a group of people are seen walki...   \n",
              "\n",
              "                                      caption_mscoco  \\\n",
              "0  An institutional looking room holds an exit si...   \n",
              "1  Black night, a blurry shot shows a tall fence ...   \n",
              "2  An older looking model computer components; tw...   \n",
              "3  A motel-like room  features all of its furnitu...   \n",
              "4  A room shows a twin bed, crib, double bed, and...   \n",
              "5  A street view shows a building entrance and tr...   \n",
              "6  Off in the distance, a few people dot the stan...   \n",
              "7  A counter top with a plate with a fork and few...   \n",
              "8  Indirect lighting in a darkened room plays acr...   \n",
              "9  the long view of a parking lot shows a row of ...   \n",
              "\n",
              "                                           embedding  \\\n",
              "0  [0.25389287, 0.2295701, -0.108173296, -0.07395...   \n",
              "1  [0.044150963, 0.17922473, -0.14461896, -0.1534...   \n",
              "2  [-0.41152698, -0.29223415, 0.025055759, -0.164...   \n",
              "3  [0.23270163, 0.07637343, 0.36209187, -0.012398...   \n",
              "4  [0.45787933, 0.13048139, 0.11703592, 0.0351443...   \n",
              "5  [0.20354465, -0.0718597, -0.27712327, -0.49626...   \n",
              "6  [-0.029054046, 0.23667967, 0.111569144, 0.0622...   \n",
              "7  [-0.29823345, 0.32871175, -0.07593347, -0.1097...   \n",
              "8  [-0.18686089, 0.35955262, 0.008806132, -0.3130...   \n",
              "9  [0.19945762, -0.1157327, -0.03143066, 0.023830...   \n",
              "\n",
              "   original_mscoco_caption_length  \\\n",
              "0                             249   \n",
              "1                             246   \n",
              "2                             244   \n",
              "3                             243   \n",
              "4                             242   \n",
              "5                             241   \n",
              "6                             235   \n",
              "7                             230   \n",
              "8                             229   \n",
              "9                             226   \n",
              "\n",
              "                                  caption_triples_ls  \n",
              "0  \"An exit sign.\"|\"Chairs.\"|\"Recording equipment...  \n",
              "1  \"In a black night, a blurry shot shows a tall ...  \n",
              "2  \"An older looking model computer components.\"|...  \n",
              "3  \"A motel-like room.\"|\"All of its furniture in ...  \n",
              "4  \"A room.\"|\"A twin bed featuring checks and pil...  \n",
              "5  \"A street view shows a building entrance.\"|\"A ...  \n",
              "6  \"A few people dot the stands bordering a ball ...  \n",
              "7  \"A counter top.\"|\"A plate with a fork and a fe...  \n",
              "8  \"Indirect lighting in a darkened room shines a...  \n",
              "9  \"A parking lot shows a row of parked vehicles....  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-437c3b43-58ee-4e97-b9f5-2362b4ca73f6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>image</th>\n",
              "      <th>caption_sharegpt4v</th>\n",
              "      <th>caption_mscoco</th>\n",
              "      <th>embedding</th>\n",
              "      <th>original_mscoco_caption_length</th>\n",
              "      <th>caption_triples_ls</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>553992</td>\n",
              "      <td>000000553992.jpg</td>\n",
              "      <td>In the image, a man and a woman are standing i...</td>\n",
              "      <td>An institutional looking room holds an exit si...</td>\n",
              "      <td>[0.25389287, 0.2295701, -0.108173296, -0.07395...</td>\n",
              "      <td>249</td>\n",
              "      <td>\"An exit sign.\"|\"Chairs.\"|\"Recording equipment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39270</td>\n",
              "      <td>000000039270.jpg</td>\n",
              "      <td>In the image, there is a man who is in the mid...</td>\n",
              "      <td>Black night, a blurry shot shows a tall fence ...</td>\n",
              "      <td>[0.044150963, 0.17922473, -0.14461896, -0.1534...</td>\n",
              "      <td>246</td>\n",
              "      <td>\"In a black night, a blurry shot shows a tall ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>277161</td>\n",
              "      <td>000000277161.jpg</td>\n",
              "      <td>In the center of the image, a vintage Apple II...</td>\n",
              "      <td>An older looking model computer components; tw...</td>\n",
              "      <td>[-0.41152698, -0.29223415, 0.025055759, -0.164...</td>\n",
              "      <td>244</td>\n",
              "      <td>\"An older looking model computer components.\"|...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>380070</td>\n",
              "      <td>000000380070.jpg</td>\n",
              "      <td>The image captures a cozy hotel room, bathed i...</td>\n",
              "      <td>A motel-like room  features all of its furnitu...</td>\n",
              "      <td>[0.23270163, 0.07637343, 0.36209187, -0.012398...</td>\n",
              "      <td>243</td>\n",
              "      <td>\"A motel-like room.\"|\"All of its furniture in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>357272</td>\n",
              "      <td>000000357272.jpg</td>\n",
              "      <td>The image captures a room that exudes a vintag...</td>\n",
              "      <td>A room shows a twin bed, crib, double bed, and...</td>\n",
              "      <td>[0.45787933, 0.13048139, 0.11703592, 0.0351443...</td>\n",
              "      <td>242</td>\n",
              "      <td>\"A room.\"|\"A twin bed featuring checks and pil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>480770</td>\n",
              "      <td>000000480770.jpg</td>\n",
              "      <td>In the heart of a bustling city, an elderly wo...</td>\n",
              "      <td>A street view shows a building entrance and tr...</td>\n",
              "      <td>[0.20354465, -0.0718597, -0.27712327, -0.49626...</td>\n",
              "      <td>241</td>\n",
              "      <td>\"A street view shows a building entrance.\"|\"A ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>515241</td>\n",
              "      <td>000000515241.jpg</td>\n",
              "      <td>The image captures a dynamic moment in a baseb...</td>\n",
              "      <td>Off in the distance, a few people dot the stan...</td>\n",
              "      <td>[-0.029054046, 0.23667967, 0.111569144, 0.0622...</td>\n",
              "      <td>235</td>\n",
              "      <td>\"A few people dot the stands bordering a ball ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>560604</td>\n",
              "      <td>000000560604.jpg</td>\n",
              "      <td>In the center of the image, a black stuffed an...</td>\n",
              "      <td>A counter top with a plate with a fork and few...</td>\n",
              "      <td>[-0.29823345, 0.32871175, -0.07593347, -0.1097...</td>\n",
              "      <td>230</td>\n",
              "      <td>\"A counter top.\"|\"A plate with a fork and a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>158005</td>\n",
              "      <td>000000158005.jpg</td>\n",
              "      <td>In the dimly lit ambiance of the room, a dinne...</td>\n",
              "      <td>Indirect lighting in a darkened room plays acr...</td>\n",
              "      <td>[-0.18686089, 0.35955262, 0.008806132, -0.3130...</td>\n",
              "      <td>229</td>\n",
              "      <td>\"Indirect lighting in a darkened room shines a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>449712</td>\n",
              "      <td>000000449712.jpg</td>\n",
              "      <td>In the image, a group of people are seen walki...</td>\n",
              "      <td>the long view of a parking lot shows a row of ...</td>\n",
              "      <td>[0.19945762, -0.1157327, -0.03143066, 0.023830...</td>\n",
              "      <td>226</td>\n",
              "      <td>\"A parking lot shows a row of parked vehicles....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-437c3b43-58ee-4e97-b9f5-2362b4ca73f6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-437c3b43-58ee-4e97-b9f5-2362b4ca73f6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-437c3b43-58ee-4e97-b9f5-2362b4ca73f6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6b4e58cb-1346-4cb1-b97c-bf1c6bac12b4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b4e58cb-1346-4cb1-b97c-bf1c6bac12b4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6b4e58cb-1346-4cb1-b97c-bf1c6bac12b4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_dc4dcf5e-7042-4ef3-b564-8fea96ff1b18\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('result_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dc4dcf5e-7042-4ef3-b564-8fea96ff1b18 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('result_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "result_df",
              "summary": "{\n  \"name\": \"result_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 174033,\n        \"min\": 39270,\n        \"max\": 560604,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          158005,\n          39270,\n          480770\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"000000158005.jpg\",\n          \"000000039270.jpg\",\n          \"000000480770.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_sharegpt4v\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"In the dimly lit ambiance of the room, a dinner table takes center stage, draped in a vibrant red tablecloth. The centerpiece of the table is a plate filled with an assortment of colorful dishes, their hues muted under the soft lighting. To the left and right of the plate are two glasses filled with water, their surfaces gleaming subtly in the light. \\n\\nOn the left side of the table, a basket filled with bread adds a rustic touch to the setting. The background fades into a dark and blurry abyss, drawing focus to the meal laid out on the table. The entire scene paints a picture of a quiet, intimate dinner, waiting to be savored.\",\n          \"In the image, there is a man who is in the midst of a powerful swing at a batting cage. He is dressed in a black shirt and is wearing a red helmet, indicating that he is prepared for the sport. The man's stance and the blur of the bat suggest that he is in action, possibly hitting a baseball.\\n\\nThe image captures him from behind and slightly to the right, providing a clear view of his swing and posture. The batting cage, surrounded by a chain link fence, is the main setting of this scene. Despite the darkness of the surroundings, one can make out faint lights and buildings in the distance, suggesting that this could be an urban setting or a sports complex.\\n\\nThe focus of the image is clearly on the batter and his action, with the background elements serving to provide context to the scene. The image does not contain any text or other discernible objects. The overall atmosphere suggests an intense practice session or perhaps even a late-night game.\",\n          \"In the heart of a bustling city, an elderly woman finds solace on a wooden bench. She's dressed in a beige jacket and blue jeans, her eyes shielded from the sun by a pair of sunglasses. The bench, her temporary haven, is surrounded by a group of pigeons, their feathers a mix of gray, black, and white. They peck at the ground, perhaps in search of food or simply enjoying the day.\\n\\nBehind the bench, a black fence stands guard, separating the woman from the world beyond. A blue car is parked on the street, its vibrant color contrasting with the muted tones of the cityscape. In the distance, a building with a red awning adds a splash of color to the scene. The woman, the pigeons, the car, and the building - each element tells a story of city life.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_mscoco\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Indirect lighting in a darkened room plays across a table covered with the remains of a meal, thereby giving everything, including the table, a basket, plates with uneaten food, utensils and glasses with liquid, a reddish patina.\",\n          \"Black night, a blurry shot shows a tall fence that angles, in a kind of Z shape, around what appears to be  a parking area  with a large building fronting it, as a kid with a red helmet takes a fierce swing in the batting area, inside the fence. \",\n          \"A street view shows a building entrance and traffic, including a bus, in the background, and to the foreground, a black iron fence, and a  bench with an older, seated person, looking at a flock of pigeons scattered on the ground and bench.  \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_mscoco_caption_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 226,\n        \"max\": 249,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          229,\n          246,\n          241\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_triples_ls\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"\\\"Indirect lighting in a darkened room shines across a table covered with the remains of a meal.\\\"|\\\"The table has a reddish patina.\\\"|\\\"The basket has a reddish patina.\\\"|\\\"The plates with uneaten food have a reddish patina.\\\"|\\\"Utensils have a reddish patina.\\\"|\\\"Glasses with liquid have a reddish patina.\\\"\",\n          \"\\\"In a black night, a blurry shot shows a tall fence that angles, in a kind of Z shape.\\\"|\\\"Around appears to be a parking area with a large building fronting it.\\\"|\\\"A kid with a red helmet takes a fierce swing in the batting area, inside the fence.\\\"\",\n          \"\\\"A street view shows a building entrance.\\\"|\\\"A street view shows traffic, including a bus in the background.\\\"|\\\"In the foreground is a black iron fence.\\\"|\\\"In the foreground is a bench with an older person sitting on it.\\\"|\\\"A flock of pigeons scattered on the ground and bench.\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependencies"
      ],
      "metadata": {
        "id": "MNNCmeITvzYx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1year9atvMY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f8748b3-3b51-4aa5-9dc7-4ad259b1e32a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "already in same group\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "already in same group\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "0\n",
            "0\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "already in same group\n",
            "already in same group\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "already in same group\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "already in same group\n",
            "0\n",
            "1\n",
            "already in same group\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "Operation took a total of 169.24766445159912 seconds.\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import ssl\n",
        "import certifi\n",
        "import time\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Settings\n",
        "MODEL = \"gpt-4o\"\n",
        "OPENAI_SECRET_KEY = \"sk-proj-X...X\"\n",
        "MAX_RETRIES = 3  # Maximum number of retries\n",
        "BATCH_SIZE = 25\n",
        "\n",
        "class UnionFind:\n",
        "    def __init__(self, size):\n",
        "        self.parent = list(range(size))\n",
        "        self.rank = [1] * size\n",
        "\n",
        "    def find(self, p):\n",
        "        if self.parent[p] != p:\n",
        "            self.parent[p] = self.find(self.parent[p])  # Path compression\n",
        "        return self.parent[p]\n",
        "\n",
        "    def union(self, p, q):\n",
        "        rootP = self.find(p)\n",
        "        rootQ = self.find(q)\n",
        "        if rootP != rootQ:\n",
        "            # Union by rank\n",
        "            if self.rank[rootP] > self.rank[rootQ]:\n",
        "                self.parent[rootQ] = rootP\n",
        "            elif self.rank[rootP] < self.rank[rootQ]:\n",
        "                self.parent[rootP] = rootQ\n",
        "            else:\n",
        "                self.parent[rootQ] = rootP\n",
        "                self.rank[rootP] += 1\n",
        "\n",
        "# Asynchronous function to check dependency\n",
        "async def check_dependency_async(session, sentence, segment1, segment2, retries=MAX_RETRIES):\n",
        "    payload = {\n",
        "        'model': MODEL,\n",
        "        'messages': [\n",
        "          {\"role\": \"system\", \"content\": \"Example 1:\\nOverall sentence: An institutional looking room holds an exit sign, chairs, recording equipment and a man with one arm aloft that appears to be demonstrating something with a device in the held-up hand for the benefit of a woman behind him that is watching closely.\\nDecomposed sentence: \\\"An exit sign.\\\"|\\\"Chairs.\\\"|\\\"Recording equipment.\\\"|\\\"A man with one arm aloft appears to be demonstrating with a device in the held-up hand.\\\"|\\\"A woman behind a man is watching him closely.\\\"\\nDependencies of decomposed sentences: 0 | 1 | 2 | 3,4\\nExample 2:\\nOverall sentence: Black night, a blurry shot shows a tall fence that angles, in a kind of Z shape, around what appears to be a parking area with a large building fronting it, as a kid with a red helmet takes a fierce swing in the batting area, inside the fence.\\nDecomposed sentence: \\\"In a black night, a blurry shot shows a tall fence that angles, in a kind of Z shape.\\\"|\\\"Around appears to be a parking area with a large building fronting it.\\\"|\\\"A kid with a red helmet takes a fierce swing in the batting area, inside the fence.\\\"\\nDependencies of decomposed sentences: 0,1,2\\nExample 3:\\nOverall sentence: An older looking model computer components; two disk drive boxes stacked, a keyboard unit, and a more modern looking computer monitor with apple logo center bottom, and on screen at top centered in light blue font letters is printed APPLE II\\nDecomposed sentence: \\\"An older looking model computer components.\\\"|\\\"Computer disk drive boxes stacked.\\\"|\\\"A keyboard unit.\\\"|\\\"A modern computer monitor with apple logo at the center bottom.\\\"|\\\"On the screen of a modern computer monitor, at the top center, light blue font letters print the words APPLE II.\\\"\\nDependencies of decomposed sentences: 0,1,2 | 3,4\\nExample 4:\\nOverall sentence: A motel-like room features all of its furniture in rows, including a sofa, double bed, stuffed chair, desk chair, desk, bureau and television, all of which, like the one piece of artwork, carpet, linen and window drapes, are all in neutrals.\\nDecomposed sentence: \\\"A motel room.\\\"|\\\"A neutral colored sofa.\\\"|\\\"A neutral colored double bed.\\\"|\\\"A neutral colored stuffed chair.\\\"|\\\"A desk chair.\\\"|\\\"A desk.\\\"|\\\"A bureau.\\\"|\\\"A television.\\\"|\\\"One piece of artwork.\\\"|\\\"Neutral colored linen.\\\"|\\\"Neutral colored window drapes.\\\"\\nDependencies of decomposed sentences: 0 | 1 | 2 | 3 | 4,5 | 6 | 7 | 8 | 9 | 10\\nExample 5:\\nOverall sentence: A room shows a twin bed, crib, double bed, and a round bed, all featuring checks and pillows, either white, or checked, high windows, a brick wall, a piece of art against a white wall, and a separate vertical surface with framed items on it.\\nDecomposed sentence: \\\"A room.\\\"|\\\"A twin bed featuring checks and pillows, either white, or checked.\\\"|\\\"A crib.\\\"|\\\"A double bed featuring checks and pillows, either white, or checked.\\\"|\\\"A round bed featuring checks and pillows, either white, or checked.\\\"|\\\"High windows.\\\"|\\\"A brick wall.\\\"|\\\"A piece of art against a white wall.\\\"|\\\"A vertical surface with framed items on it.\\\"\\nDependencies of decomposed sentences: 0 | 1| 2 | 3 | 4 | 5 | 6 | 7 | 8\\nExample 6:\\nOverall sentence: A street view shows a building entrance and traffic, including a bus, in the background, and to the foreground, a black iron fence, and a bench with an older, seated person, looking at a flock of pigeons scattered on the ground and bench.\\nDecomposed sentence: \\\"A street view shows a building entrance.\\\"|\\\"A street view shows traffic, including a bus in the background.\\\"|\\\"In the foreground is a black iron fence.\\\"|\\\"In the foreground is a bench with an older person sitting on it\\\"|\\\"A flock of pigeons scattered on the ground and bench.\\\"\\nDependencies of decomposed sentences: 0,1 | 2 | 3,4\\nExample 7:\\nOverall sentence: Off in the distance, a few people dot the stands and the fenced in area bordering a ball field, while an umpire and three uniformed baseball players, including a batter, catcher and outfielder, strike playing poses in the foreground.\\nDecomposed form: \\\"A few people dot the stands bordering a ball field.\\\"|\\\"A few people dot the fenced-in area bordering a ball field.\\\"|\\\"A baseball umpire.\\\"|\\\"A baseball batter.\\\"|\\\"A baseball catcher.\\\"|\\\"An outfielder.\\\"\\nDependencies of decomposed sentences: 0 | 1 | 2,3,4,5\\nExample 8:\\nOverall sentence: A counter top with a plate with a fork and few scraps of food and a teddy bear lying on side with arm outstretched on plate near fork, with another plate with an apple and two bowls with produce, a canister and some metal objects.\\nDecomposed sentence: \\\"A counter top.\\\"|\\\"A plate with a fork and few scraps of food.\\\"|\\\"A teddy bear lying with arm outstretched on plate near fork.\\\"|\\\"A plate with an apple.\\\"|\\\"Bowls with apples.\\\"|\\\"A canister.\\\"|\\\"Metal objects.\\\"\\nDependencies of decomposed sentences: 0 | 1,2 | 3 | 4 | 5 | 6\\nExample 9:\\nOverall sentence: Indirect lighting in a darkened room plays across a table covered with the remains of a meal, thereby giving everything, including the table, a basket, plates with uneaten food, utensils and glasses with liquid, a reddish patina.\\nDecomposed sentence: \\\"Indirect lighting in a darkened room shines across a table covered with the remains of a meal.\\\"|\\\"The table has a reddish patina.\\\"|\\\"The basket has a reddish patina.\\\"|\\\"The plates with uneaten food have a reddish patina.\\\"|\\\"Utensils have a reddish patina.\\\"|\\\"Glasses with liquid have a reddish patina.\\\"\\nDependencies of decomposed sentences: 0,1,2,3,4,5\\nExample 10:\\nOverall sentence: the long view of a parking lot shows a row of parked vehicles, including a bus, which appears to be the destination of a number of similarly dressed women with luggage that are moving towards it from the immediate foreground.\\nDecomposed sentence: \\\"A parking lot shows a row of parked vehicles.\\\"|\\\"A bus.\\\"|\\\"Similarly dressed women with luggages.\\\"\\nDependencies of decomposed sentences: 0,1 | 2\\nExample 11:\\nOverall sentence: Corner view of a table with plaid cloth, red and white checkerboard napkins, a pizza, a container of green leafy vegetable, and some bottles, with the hips of a person wearing an apron standing beside table in green grass.\\nDecomposed sentence: \\\"Corner view of a table with plaid cloth.\\\"|\\\"Red and white checkerboard napkins.\\\"|\\\"A pizza.\\\"|\\\"A container of green leafy vegetables.\\\"|\\\"Some bottles.\\\"|\\\"The hips of a person wearing an apron standing beside a table in green grass.\\\"\\nDependencies of decomposed sentences: 0,5| 1 | 2 | 3 | 4\\nExample 12:\\nOverall sentence: A room with natural, unfinished, pale wood walls and built in shelves, also has curtained sliding doors that open onto a patio with a lounge chair, a big double bed with a blue blanket, and various framed photos and books.\\nDecomposed sentence: \\\"A room.\\\"|\\\"Natural, unfinished, pale wood walls.\\\"|\\\"Built-in shelves.\\\"|\\\"Curtained sliding doors.\\\"|\\\"Sliding doors that open onto a patio with a lounge chair.\\\"|\\\"A big double bed with a blue blanket.\\\"|\\\"Framed photos.\\\"|\\\"Books.\\\"\\nDependencies of decomposed sentences: 0 | 1 | 2 | 3,4 | 5 | 6 | 7\\nExample 13:\\nOverall sentence: A minimally furnished room shows small window pairs with blinds and no curtains, an unmade double bed with a heavy, taupe duvet, and a white border near the ceiling with colorful birds trailing in both directions.\\nDecomposed sentence: \\\"A minimally furnished room.\\\"|\\\"Small window pairs with blinds and no curtains.\\\"|\\\"An unmade double bed with a heavy, taupe duvet.\\\"|\\\"A white border near the ceiling with colorful birds trailing in both directions.\\\"\\nDependencies of decomposed sentences: 0 | 1 | 2 | 3\\nExample 14:\\nOverall sentence: A blurry night-time street scene shows a higher and lower roadway with overpasses and lots of traffic going the same way, while to the left an area of red glare with an arrow and number, suggests a caution area.\\nDecomposed sentence: \\\"A blurry night-time street scene shows a higher and lower roadway with overpasses and lots of traffic going the same way.\\\"|\\\"An area of red glare with an arrow and number, suggests a caution area.\\\"\\nDependencies of decomposed sentences: 0,1\\nExample 15:\\nOverall sentence: Several people are standing, or sitting, in the vicinity of a scoreboard, to the front of which lies a tennis court, featuring a player with his knees bent, who is also grasping a racquet with both hands,\\nDecomposed sentence: \\\"Several people are standing, or sitting, in the vicinity of a scoreboard.\\\"|\\\"In front of the scoreboard lies a tennis court.\\\"|\\\"A tennis player with his knees bent, who is grasping a racquet with both hands.\\\"\\nDependencies of decomposed sentences: 0,1 | 2\\nExample 16:\\nOverall sentence: Black and white of a blacksmith interior with a horse, a man standing with arm out to it, and a man ducking under it's head and holding a long metal file in both hands, with horses on poles on the wall.\\nDecomposed sentence: \\\"Black and white photo of a blacksmith interior with a horse.\\\"|\\\"Black and white photo of a man standing with arm out to a horse.\\\"|\\\"Black and white photo of a man ducking under a horses head and holding a long metal file in both hands.\\\"|\\\"Black and white photo with horse poles on the wall.\\\"\\nDependencies of decomposed sentences: 0,1,2 | 3\\nExample 17:\\nOverall sentence: A blurry living room scene suggests a wide screen T,V. with a game playing, bookshelves, a low cluttered table, a couch, and closest and clearest, a sports t-shirt with the number thirty-five on it.\\nDecomposed sentence: \\\"A blurry living room scene.\\\"|\\\"A wide screen TV with a sports game playing.\\\"|\\\"Bookshelves.\\\"|\\\"A low cluttered table.\\\"|\\\"A couch.\\\"|\\\"A sports t-shirt with the number 35 on it.\\\"\\nDependencies of decomposed sentences: 0 | 1 | 2 | 3 | 4 | 5\\nExample 18:\\nOverall sentence: In the first picture a boy is flipping his skateboard, in the second picture is skating and lifting the board slightly off the ground, and in the third he is jumping into the air onto the skateboard.\\nDecomposed sentence: \\\"A boy is flipping his skateboard.\\\"|\\\"A boy is skating and lifting the skateboard slightly off the ground.\\\"|\\\"A boy is jumping into the air onto the skateboard.\\\"\\nDependencies of decomposed sentences: 0 | 1 | 2\\nExample 19:\\nOverall sentence: A table with a person holding a sandwich in hand over a plate with a knife on it, and another plate with a knife, napkin, and french bread sandwich, along with a bottle and a glass with beverages.\\nDecomposed sentence: \\\"A table with a person holding a sandwich in hand.\\\"|\\\"A sandwich in hand over a plate with a knife on it.\\\"|\\\"A plate with a knife, napkin, and french bread sandwich.\\\"|\\\"A bottle.\\\"|\\\"A glass with beverages.\\\"\\nDependencies of decomposed sentences: 0,1 | 2,3,4\\nExample 20:\\nOverall sentence: An impressive array of motorcycles recedes into the distance between a driveway and a building fronted with cars and a grassy area, as lots of people mill about the driveway, around the bikes.\\nDecomposed sentence: \\\"An array of motorcycles between a driveway and a building.\\\"|\\\"A building fronted with cars and a grassy area.\\\"|\\\"Lots of people mill about the driveway, around the bikes.\\\"\\nDependencies of decomposed sentences: 0,1,2\\nLearning from how the provided 20 examples of overall sentences have their decomposed phrases, each represented by their respective index, starting from 0, are decomposed, then the decomposed phrases are grouped together by commas to indicate phrases are dependent, while bars separate independent phrases or phrase groups, output whether two decomposed phrases are dependent given the overall sentence. You may answer in only one digit, 1 meaning yes, 0 meaning no.\"},\n",
        "          {\"role\": \"user\", \"content\": f\"Overall sentence:{sentence}\\n And two decomposed phrases: {segment1}|{segment2}\\nPlease output 1 or 0, 1 meaning they are dependent phrases, 0 meaning they are independent phraes.\"}\n",
        "        ],\n",
        "        'max_tokens': 1,\n",
        "        'n': 1,\n",
        "        'temperature': 0\n",
        "    }\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            async with session.post(\n",
        "                url='https://api.openai.com/v1/chat/completions',\n",
        "                headers={\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {OPENAI_SECRET_KEY}\"},\n",
        "                json=payload,\n",
        "                ssl=ssl.create_default_context(cafile=certifi.where())\n",
        "            ) as response:\n",
        "                response_json = await response.json()\n",
        "            if \"error\" in response_json:\n",
        "                print(f\"OpenAI request failed with error {response_json['error']}\")\n",
        "                if response_json['error']['code'] == 'rate_limit_exceeded':\n",
        "                    retry_after = 10\n",
        "                    await asyncio.sleep(retry_after)  # Wait for the retry-after duration before retrying\n",
        "                continue\n",
        "            result = response_json['choices'][0]['message']['content']\n",
        "            if result in {'1', '0'}:\n",
        "                print(result)\n",
        "                return int(result)\n",
        "            else:\n",
        "                print(\"errors\")\n",
        "                return 0\n",
        "        except Exception as e:\n",
        "            print(f\"Request failed: {e}\")\n",
        "    print(\"errors\")\n",
        "    return 0\n",
        "\n",
        "# Asynchronous function to group dependent segments for a single sentence\n",
        "async def group_dependent_segments_async(session, sentence, segments):\n",
        "    start_time = time.time()  # Start time\n",
        "    segments_list = segments.split('|')\n",
        "    n = len(segments_list)\n",
        "\n",
        "    # Initialize Union-Find\n",
        "    uf = UnionFind(n)\n",
        "\n",
        "    # Perform pairwise comparisons sequentially\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            if uf.find(i) != uf.find(j):  # Check if i and j are already in the same group\n",
        "                result = await check_dependency_async(session, sentence, segments_list[i], segments_list[j])\n",
        "                if result == 1:\n",
        "                    uf.union(i, j)\n",
        "                await asyncio.sleep(2)  # Small delay to avoid hitting limits too quickly\n",
        "            else:\n",
        "              print('already in same group')\n",
        "\n",
        "    # Group segments based on union-find results\n",
        "    groups = {}\n",
        "    for i in range(n):\n",
        "        root = uf.find(i)\n",
        "        if root not in groups:\n",
        "            groups[root] = []\n",
        "        groups[root].append(i)\n",
        "\n",
        "    # Convert indices back to segments\n",
        "    grouped_indices = [\",\".join(map(str, sorted(group))) for group in groups.values()]\n",
        "    end_time = time.time()  # End time\n",
        "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
        "    return \" | \".join(grouped_indices), elapsed_time\n",
        "\n",
        "# Asynchronous function to process multiple sentences in parallel\n",
        "async def process_sentences_in_bulk(sentences_and_segments):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        results = []\n",
        "        for i in range(0, len(sentences_and_segments), BATCH_SIZE):\n",
        "            batch = sentences_and_segments[i:i+BATCH_SIZE]\n",
        "            tasks = [group_dependent_segments_async(session, sentence, segments) for sentence, segments in batch]\n",
        "            batch_results = await asyncio.gather(*tasks)\n",
        "            results.extend(batch_results)\n",
        "            if i + BATCH_SIZE < len(sentences_and_segments):\n",
        "                await asyncio.sleep(10)  # Wait for 60 seconds between batches to respect rate limits\n",
        "    return results\n",
        "\n",
        "def apply_to_dataframe(df, sentence_col, segments_col):\n",
        "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    df_copy = df.copy()\n",
        "    sentences_and_segments = list(zip(df_copy[sentence_col], df_copy[segments_col]))\n",
        "    results = asyncio.run(process_sentences_in_bulk(sentences_and_segments))\n",
        "    df_copy['groups'] = [result[0] for result in results]\n",
        "    df_copy['elapsed_time'] = [result[1] for result in results]\n",
        "    return df_copy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    begin = time.time()\n",
        "    df = intermediate_df.copy() #can change according to user\n",
        "    # Filter rows that do not start and end with \"\n",
        "    filtered_df = df[~(df['caption_triples_ls'].str.startswith('\"') & df['caption_triples_ls'].str.endswith('\"'))].copy()\n",
        "    # Apply the lambda function to those rows only\n",
        "    filtered_df['caption_triples_ls'] = filtered_df['caption_triples_ls'].apply(lambda x: '\"' + x.split('\"', 1)[-1].rsplit('\"', 1)[0] + '\"')\n",
        "    # Combine the processed rows back with the original DataFrame\n",
        "    result_df = pd.concat([df[(df['caption_triples_ls'].str.startswith('\"') & df['caption_triples_ls'].str.endswith('\"'))], filtered_df]).sort_index()\n",
        "    # Apply function to DataFrame\n",
        "    final_df = apply_to_dataframe(result_df, 'caption_mscoco', 'caption_triples_ls')\n",
        "    end = time.time()\n",
        "    total_time = end - begin\n",
        "    print(f\"Operation took a total of {total_time} seconds.\")\n",
        "    '''\n",
        "    pickle_file_path = '/content/drive/MyDrive/mscoco_500decomposed_dependencies.pkl'\n",
        "    with open(pickle_file_path, 'wb') as f:\n",
        "      pickle.dump(final_df, f)\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "kXcIn9ZVIWzc",
        "outputId": "9f150f48-638b-4398-b131-26422d374330"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id             image  \\\n",
              "0  553992  000000553992.jpg   \n",
              "1   39270  000000039270.jpg   \n",
              "2  277161  000000277161.jpg   \n",
              "3  380070  000000380070.jpg   \n",
              "4  357272  000000357272.jpg   \n",
              "5  480770  000000480770.jpg   \n",
              "6  515241  000000515241.jpg   \n",
              "7  560604  000000560604.jpg   \n",
              "8  158005  000000158005.jpg   \n",
              "9  449712  000000449712.jpg   \n",
              "\n",
              "                                  caption_sharegpt4v  \\\n",
              "0  In the image, a man and a woman are standing i...   \n",
              "1  In the image, there is a man who is in the mid...   \n",
              "2  In the center of the image, a vintage Apple II...   \n",
              "3  The image captures a cozy hotel room, bathed i...   \n",
              "4  The image captures a room that exudes a vintag...   \n",
              "5  In the heart of a bustling city, an elderly wo...   \n",
              "6  The image captures a dynamic moment in a baseb...   \n",
              "7  In the center of the image, a black stuffed an...   \n",
              "8  In the dimly lit ambiance of the room, a dinne...   \n",
              "9  In the image, a group of people are seen walki...   \n",
              "\n",
              "                                      caption_mscoco  \\\n",
              "0  An institutional looking room holds an exit si...   \n",
              "1  Black night, a blurry shot shows a tall fence ...   \n",
              "2  An older looking model computer components; tw...   \n",
              "3  A motel-like room  features all of its furnitu...   \n",
              "4  A room shows a twin bed, crib, double bed, and...   \n",
              "5  A street view shows a building entrance and tr...   \n",
              "6  Off in the distance, a few people dot the stan...   \n",
              "7  A counter top with a plate with a fork and few...   \n",
              "8  Indirect lighting in a darkened room plays acr...   \n",
              "9  the long view of a parking lot shows a row of ...   \n",
              "\n",
              "                                           embedding  \\\n",
              "0  [0.25389287, 0.2295701, -0.108173296, -0.07395...   \n",
              "1  [0.044150963, 0.17922473, -0.14461896, -0.1534...   \n",
              "2  [-0.41152698, -0.29223415, 0.025055759, -0.164...   \n",
              "3  [0.23270163, 0.07637343, 0.36209187, -0.012398...   \n",
              "4  [0.45787933, 0.13048139, 0.11703592, 0.0351443...   \n",
              "5  [0.20354465, -0.0718597, -0.27712327, -0.49626...   \n",
              "6  [-0.029054046, 0.23667967, 0.111569144, 0.0622...   \n",
              "7  [-0.29823345, 0.32871175, -0.07593347, -0.1097...   \n",
              "8  [-0.18686089, 0.35955262, 0.008806132, -0.3130...   \n",
              "9  [0.19945762, -0.1157327, -0.03143066, 0.023830...   \n",
              "\n",
              "   original_mscoco_caption_length  \\\n",
              "0                             249   \n",
              "1                             246   \n",
              "2                             244   \n",
              "3                             243   \n",
              "4                             242   \n",
              "5                             241   \n",
              "6                             235   \n",
              "7                             230   \n",
              "8                             229   \n",
              "9                             226   \n",
              "\n",
              "                                  caption_triples_ls  \\\n",
              "0  \"An exit sign.\"|\"Chairs.\"|\"Recording equipment...   \n",
              "1  \"In a black night, a blurry shot shows a tall ...   \n",
              "2  \"An older looking model computer components.\"|...   \n",
              "3  \"A motel-like room.\"|\"All of its furniture in ...   \n",
              "4  \"A room.\"|\"A twin bed featuring checks and pil...   \n",
              "5  \"A street view shows a building entrance.\"|\"A ...   \n",
              "6  \"A few people dot the stands bordering a ball ...   \n",
              "7  \"A counter top.\"|\"A plate with a fork and a fe...   \n",
              "8  \"Indirect lighting in a darkened room shines a...   \n",
              "9  \"A parking lot shows a row of parked vehicles....   \n",
              "\n",
              "                               groups  elapsed_time  \n",
              "0                     0 | 1 | 2 | 3,4     30.251717  \n",
              "1                               0,1,2      9.223561  \n",
              "2                           0,1,2,3,4     26.510803  \n",
              "3  0,1,2,3,4,9,10,11,12 | 5 | 6 | 7,8    169.059217  \n",
              "4         0 | 1,3,4 | 2 | 5 | 6,7 | 8    102.877314  \n",
              "5                         0,1 | 2,3,4     29.058734  \n",
              "6                       0,1 | 2,3,4,5     34.213959  \n",
              "7                     0,1,2 | 3,4,5,6     59.806925  \n",
              "8                         0,1,2,3,4,5     20.641703  \n",
              "9                               0,1,2      6.220107  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a16a9c7-df3a-4292-9c02-d79346b09c49\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>image</th>\n",
              "      <th>caption_sharegpt4v</th>\n",
              "      <th>caption_mscoco</th>\n",
              "      <th>embedding</th>\n",
              "      <th>original_mscoco_caption_length</th>\n",
              "      <th>caption_triples_ls</th>\n",
              "      <th>groups</th>\n",
              "      <th>elapsed_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>553992</td>\n",
              "      <td>000000553992.jpg</td>\n",
              "      <td>In the image, a man and a woman are standing i...</td>\n",
              "      <td>An institutional looking room holds an exit si...</td>\n",
              "      <td>[0.25389287, 0.2295701, -0.108173296, -0.07395...</td>\n",
              "      <td>249</td>\n",
              "      <td>\"An exit sign.\"|\"Chairs.\"|\"Recording equipment...</td>\n",
              "      <td>0 | 1 | 2 | 3,4</td>\n",
              "      <td>30.251717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39270</td>\n",
              "      <td>000000039270.jpg</td>\n",
              "      <td>In the image, there is a man who is in the mid...</td>\n",
              "      <td>Black night, a blurry shot shows a tall fence ...</td>\n",
              "      <td>[0.044150963, 0.17922473, -0.14461896, -0.1534...</td>\n",
              "      <td>246</td>\n",
              "      <td>\"In a black night, a blurry shot shows a tall ...</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>9.223561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>277161</td>\n",
              "      <td>000000277161.jpg</td>\n",
              "      <td>In the center of the image, a vintage Apple II...</td>\n",
              "      <td>An older looking model computer components; tw...</td>\n",
              "      <td>[-0.41152698, -0.29223415, 0.025055759, -0.164...</td>\n",
              "      <td>244</td>\n",
              "      <td>\"An older looking model computer components.\"|...</td>\n",
              "      <td>0,1,2,3,4</td>\n",
              "      <td>26.510803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>380070</td>\n",
              "      <td>000000380070.jpg</td>\n",
              "      <td>The image captures a cozy hotel room, bathed i...</td>\n",
              "      <td>A motel-like room  features all of its furnitu...</td>\n",
              "      <td>[0.23270163, 0.07637343, 0.36209187, -0.012398...</td>\n",
              "      <td>243</td>\n",
              "      <td>\"A motel-like room.\"|\"All of its furniture in ...</td>\n",
              "      <td>0,1,2,3,4,9,10,11,12 | 5 | 6 | 7,8</td>\n",
              "      <td>169.059217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>357272</td>\n",
              "      <td>000000357272.jpg</td>\n",
              "      <td>The image captures a room that exudes a vintag...</td>\n",
              "      <td>A room shows a twin bed, crib, double bed, and...</td>\n",
              "      <td>[0.45787933, 0.13048139, 0.11703592, 0.0351443...</td>\n",
              "      <td>242</td>\n",
              "      <td>\"A room.\"|\"A twin bed featuring checks and pil...</td>\n",
              "      <td>0 | 1,3,4 | 2 | 5 | 6,7 | 8</td>\n",
              "      <td>102.877314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>480770</td>\n",
              "      <td>000000480770.jpg</td>\n",
              "      <td>In the heart of a bustling city, an elderly wo...</td>\n",
              "      <td>A street view shows a building entrance and tr...</td>\n",
              "      <td>[0.20354465, -0.0718597, -0.27712327, -0.49626...</td>\n",
              "      <td>241</td>\n",
              "      <td>\"A street view shows a building entrance.\"|\"A ...</td>\n",
              "      <td>0,1 | 2,3,4</td>\n",
              "      <td>29.058734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>515241</td>\n",
              "      <td>000000515241.jpg</td>\n",
              "      <td>The image captures a dynamic moment in a baseb...</td>\n",
              "      <td>Off in the distance, a few people dot the stan...</td>\n",
              "      <td>[-0.029054046, 0.23667967, 0.111569144, 0.0622...</td>\n",
              "      <td>235</td>\n",
              "      <td>\"A few people dot the stands bordering a ball ...</td>\n",
              "      <td>0,1 | 2,3,4,5</td>\n",
              "      <td>34.213959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>560604</td>\n",
              "      <td>000000560604.jpg</td>\n",
              "      <td>In the center of the image, a black stuffed an...</td>\n",
              "      <td>A counter top with a plate with a fork and few...</td>\n",
              "      <td>[-0.29823345, 0.32871175, -0.07593347, -0.1097...</td>\n",
              "      <td>230</td>\n",
              "      <td>\"A counter top.\"|\"A plate with a fork and a fe...</td>\n",
              "      <td>0,1,2 | 3,4,5,6</td>\n",
              "      <td>59.806925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>158005</td>\n",
              "      <td>000000158005.jpg</td>\n",
              "      <td>In the dimly lit ambiance of the room, a dinne...</td>\n",
              "      <td>Indirect lighting in a darkened room plays acr...</td>\n",
              "      <td>[-0.18686089, 0.35955262, 0.008806132, -0.3130...</td>\n",
              "      <td>229</td>\n",
              "      <td>\"Indirect lighting in a darkened room shines a...</td>\n",
              "      <td>0,1,2,3,4,5</td>\n",
              "      <td>20.641703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>449712</td>\n",
              "      <td>000000449712.jpg</td>\n",
              "      <td>In the image, a group of people are seen walki...</td>\n",
              "      <td>the long view of a parking lot shows a row of ...</td>\n",
              "      <td>[0.19945762, -0.1157327, -0.03143066, 0.023830...</td>\n",
              "      <td>226</td>\n",
              "      <td>\"A parking lot shows a row of parked vehicles....</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>6.220107</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a16a9c7-df3a-4292-9c02-d79346b09c49')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a16a9c7-df3a-4292-9c02-d79346b09c49 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a16a9c7-df3a-4292-9c02-d79346b09c49');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-68775312-7905-4f10-8368-4d4eeb9c05a1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-68775312-7905-4f10-8368-4d4eeb9c05a1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-68775312-7905-4f10-8368-4d4eeb9c05a1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_4fd860b7-cea9-4bb3-8b5d-6b3d1b6f8ca0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('final_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4fd860b7-cea9-4bb3-8b5d-6b3d1b6f8ca0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('final_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "final_df",
              "summary": "{\n  \"name\": \"final_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 174033,\n        \"min\": 39270,\n        \"max\": 560604,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          158005,\n          39270,\n          480770\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"000000158005.jpg\",\n          \"000000039270.jpg\",\n          \"000000480770.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_sharegpt4v\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"In the dimly lit ambiance of the room, a dinner table takes center stage, draped in a vibrant red tablecloth. The centerpiece of the table is a plate filled with an assortment of colorful dishes, their hues muted under the soft lighting. To the left and right of the plate are two glasses filled with water, their surfaces gleaming subtly in the light. \\n\\nOn the left side of the table, a basket filled with bread adds a rustic touch to the setting. The background fades into a dark and blurry abyss, drawing focus to the meal laid out on the table. The entire scene paints a picture of a quiet, intimate dinner, waiting to be savored.\",\n          \"In the image, there is a man who is in the midst of a powerful swing at a batting cage. He is dressed in a black shirt and is wearing a red helmet, indicating that he is prepared for the sport. The man's stance and the blur of the bat suggest that he is in action, possibly hitting a baseball.\\n\\nThe image captures him from behind and slightly to the right, providing a clear view of his swing and posture. The batting cage, surrounded by a chain link fence, is the main setting of this scene. Despite the darkness of the surroundings, one can make out faint lights and buildings in the distance, suggesting that this could be an urban setting or a sports complex.\\n\\nThe focus of the image is clearly on the batter and his action, with the background elements serving to provide context to the scene. The image does not contain any text or other discernible objects. The overall atmosphere suggests an intense practice session or perhaps even a late-night game.\",\n          \"In the heart of a bustling city, an elderly woman finds solace on a wooden bench. She's dressed in a beige jacket and blue jeans, her eyes shielded from the sun by a pair of sunglasses. The bench, her temporary haven, is surrounded by a group of pigeons, their feathers a mix of gray, black, and white. They peck at the ground, perhaps in search of food or simply enjoying the day.\\n\\nBehind the bench, a black fence stands guard, separating the woman from the world beyond. A blue car is parked on the street, its vibrant color contrasting with the muted tones of the cityscape. In the distance, a building with a red awning adds a splash of color to the scene. The woman, the pigeons, the car, and the building - each element tells a story of city life.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_mscoco\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Indirect lighting in a darkened room plays across a table covered with the remains of a meal, thereby giving everything, including the table, a basket, plates with uneaten food, utensils and glasses with liquid, a reddish patina.\",\n          \"Black night, a blurry shot shows a tall fence that angles, in a kind of Z shape, around what appears to be  a parking area  with a large building fronting it, as a kid with a red helmet takes a fierce swing in the batting area, inside the fence. \",\n          \"A street view shows a building entrance and traffic, including a bus, in the background, and to the foreground, a black iron fence, and a  bench with an older, seated person, looking at a flock of pigeons scattered on the ground and bench.  \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_mscoco_caption_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 226,\n        \"max\": 249,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          229,\n          246,\n          241\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_triples_ls\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"\\\"Indirect lighting in a darkened room shines across a table covered with the remains of a meal.\\\"|\\\"The table has a reddish patina.\\\"|\\\"The basket has a reddish patina.\\\"|\\\"The plates with uneaten food have a reddish patina.\\\"|\\\"Utensils have a reddish patina.\\\"|\\\"Glasses with liquid have a reddish patina.\\\"\",\n          \"\\\"In a black night, a blurry shot shows a tall fence that angles, in a kind of Z shape.\\\"|\\\"Around appears to be a parking area with a large building fronting it.\\\"|\\\"A kid with a red helmet takes a fierce swing in the batting area, inside the fence.\\\"\",\n          \"\\\"A street view shows a building entrance.\\\"|\\\"A street view shows traffic, including a bus in the background.\\\"|\\\"In the foreground is a black iron fence.\\\"|\\\"In the foreground is a bench with an older person sitting on it.\\\"|\\\"A flock of pigeons scattered on the ground and bench.\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"groups\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"0,1,2 | 3,4,5,6\",\n          \"0,1,2\",\n          \"0,1 | 2,3,4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"elapsed_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50.68786236025391,\n        \"min\": 6.220107316970825,\n        \"max\": 169.05921697616577,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          20.64170265197754,\n          9.22356104850769,\n          29.05873417854309\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V2"
      ],
      "metadata": {
        "id": "J0QIrLrzttHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import ssl\n",
        "import certifi\n",
        "import time\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Settings\n",
        "MODEL = \"gpt-4o\"\n",
        "OPENAI_SECRET_KEY = \"sk-proj-X...X\"\n",
        "MAX_RETRIES = 3  # Maximum number of retries\n",
        "BATCH_SIZE = 35\n",
        "\n",
        "class UnionFind:\n",
        "    def __init__(self, size):\n",
        "        self.parent = list(range(size))\n",
        "        self.rank = [1] * size\n",
        "\n",
        "    def find(self, p):\n",
        "        if self.parent[p] != p:\n",
        "            self.parent[p] = self.find(self.parent[p])  # Path compression\n",
        "        return self.parent[p]\n",
        "\n",
        "    def union(self, p, q):\n",
        "        rootP = self.find(p)\n",
        "        rootQ = self.find(q)\n",
        "        if rootP != rootQ:\n",
        "            # Union by rank\n",
        "            if self.rank[rootP] > self.rank[rootQ]:\n",
        "                self.parent[rootQ] = rootP\n",
        "            elif self.rank[rootP] < self.rank[rootQ]:\n",
        "                self.parent[rootP] = rootQ\n",
        "            else:\n",
        "                self.parent[rootQ] = rootP\n",
        "                self.rank[rootP] += 1\n",
        "\n",
        "# Asynchronous function to check dependency\n",
        "async def check_dependency_async(session, sentence, segment1, segment2, retries=MAX_RETRIES):\n",
        "    payload = {\n",
        "        'model': MODEL,\n",
        "        'messages': [\n",
        "          {\"role\": \"system\", \"content\": \"Example 1:\\nOverall sentence: An institutional looking room holds an exit sign, chairs, recording equipment and a man with one arm aloft that appears to be demonstrating something with a device in the held-up hand for the benefit of a woman behind him that is watching closely.\\nDecomposed sentence: [\\\"An exit sign.\\\"]|[\\\"Chairs.\\\"]|[\\\"Recording equipment.\\\"]|[\\\"A man with one arm aloft appears to be demonstrating with a device in the held-up hand.\\\",\\\"A woman behind a man is watching him closely.\\\"]\\nExample 2:\\nOverall sentence: Black night, a blurry shot shows a tall fence that angles, in a kind of Z shape, around what appears to be a parking area with a large building fronting it, as a kid with a red helmet takes a fierce swing in the batting area, inside the fence.\\nDecomposed sentence: [\\\"In a black night, a blurry shot shows a tall fence that angles, in a kind of Z shape.\\\",\\\"Around appears to be a parking area with a large building fronting it.\\\",\\\"A kid with a red helmet takes a fierce swing in the batting area, inside the fence.\\\"]\\nExample 3:\\nOverall sentence: An older looking model computer components; two disk drive boxes stacked, a keyboard unit, and a more modern looking computer monitor with apple logo center bottom, and on screen at top centered in light blue font letters is printed APPLE II\\nDecomposed sentence: [\\\"An older looking model computer components.\\\",\\\"Computer disk drive boxes stacked.\\\",\\\"A keyboard unit.\\\"]|[\\\"A modern computer monitor with apple logo at the center bottom.\\\",\\\"On the screen of a modern computer monitor, at the top center, light blue font letters print the words APPLE II.\\\"]\\nExample 4:\\nOverall sentence: A motel-like room features all of its furniture in rows, including a sofa, double bed, stuffed chair, desk chair, desk, bureau and television, all of which, like the one piece of artwork, carpet, linen and window drapes, are all in neutrals.\\nDecomposed sentence: [\\\"A motel room.\\\"]|[\\\"A neutral colored sofa.\\\"]|[\\\"A neutral colored double bed.\\\"]|[\\\"A neutral colored stuffed chair.\\\"]|[\\\"A desk chair.\\\",\\\"A desk.\\\"]|[\\\"A bureau.\\\"]|[\\\"A television.\\\"]|[\\\"One piece of artwork.\\\"]|[\\\"Neutral colored linen.\\\"]|[\\\"Neutral colored window drapes.\\\"]\\nExample 5:\\nOverall sentence: A room shows a twin bed, crib, double bed, and a round bed, all featuring checks and pillows, either white, or checked, high windows, a brick wall, a piece of art against a white wall, and a separate vertical surface with framed items on it.\\nDecomposed sentence: [\\\"A room.\\\"]|[\\\"A twin bed featuring checks and pillows, either white, or checked.\\\"]|[\\\"A crib.\\\"]|[\\\"A double bed featuring checks and pillows, either white, or checked.\\\"]|[\\\"A round bed featuring checks and pillows, either white, or checked.\\\"]|[\\\"High windows.\\\"]|[\\\"A brick wall.\\\"]|[\\\"A piece of art against a white wall.\\\"]|[\\\"A vertical surface with framed items on it.\\\"]\\nExample 6:\\nOverall sentence: A street view shows a building entrance and traffic, including a bus, in the background, and to the foreground, a black iron fence, and a bench with an older, seated person, looking at a flock of pigeons scattered on the ground and bench.\\nDecomposed sentence: [\\\"A street view shows a building entrance.\\\",\\\"A street view shows traffic, including a bus in the background.\\\"]|[\\\"In the foreground is a black iron fence.\\\"]|[\\\"In the foreground is a bench with an older person sitting on it\\\",\\\"A flock of pigeons scattered on the ground and bench.\\\"]\\nExample 7:\\nOverall sentence: Off in the distance, a few people dot the stands and the fenced in area bordering a ball field, while an umpire and three uniformed baseball players, including a batter, catcher and outfielder, strike playing poses in the foreground.\\nDecomposed form: [\\\"A few people dot the stands bordering a ball field.\\\"]|[\\\"A few people dot the fenced-in area bordering a ball field.\\\"]|[\\\"A baseball umpire.\\\",\\\"A baseball batter.\\\",\\\"A baseball catcher.\\\",\\\"An outfielder.\\\"]\\nExample 8:\\nOverall sentence: A counter top with a plate with a fork and few scraps of food and a teddy bear lying on side with arm outstretched on plate near fork, with another plate with an apple and two bowls with produce, a canister and some metal objects.\\nDecomposed sentence: [\\\"A counter top.\\\"]|[\\\"A plate with a fork and few scraps of food.\\\",\\\"A teddy bear lying with arm outstretched on plate near fork.\\\"]|[\\\"A plate with an apple.\\\"]|[\\\"Bowls with apples.\\\"]|[\\\"A canister.\\\"]|[\\\"Metal objects.\\\"]\\nExample 9:\\nOverall sentence: Indirect lighting in a darkened room plays across a table covered with the remains of a meal, thereby giving everything, including the table, a basket, plates with uneaten food, utensils and glasses with liquid, a reddish patina.\\nDecomposed sentence: [\\\"Indirect lighting in a darkened room shines across a table covered with the remains of a meal.\\\",\\\"The table has a reddish patina.\\\",\\\"The basket has a reddish patina.\\\",\\\"The plates with uneaten food have a reddish patina.\\\",\\\"Utensils have a reddish patina.\\\",\\\"Glasses with liquid have a reddish patina.\\\"]\\nExample 10:\\nOverall sentence: the long view of a parking lot shows a row of parked vehicles, including a bus, which appears to be the destination of a number of similarly dressed women with luggage that are moving towards it from the immediate foreground.\\nDecomposed sentence: [\\\"A parking lot shows a row of parked vehicles.\\\",\\\"A bus.\\\"]|[\\\"Similarly dressed women with luggages.\\\"]\\nExample 11:\\nOverall sentence: Corner view of a table with plaid cloth, red and white checkerboard napkins, a pizza, a container of green leafy vegetable, and some bottles, with the hips of a person wearing an apron standing beside table in green grass.\\nDecomposed sentence: [\\\"Corner view of a table with plaid cloth.\\\",\\\"The hips of a person wearing an apron standing beside a table in green grass.\\\"]|[\\\"Red and white checkerboard napkins.\\\"]|[\\\"A pizza.\\\"]|[\\\"A container of green leafy vegetables.\\\"]|[\\\"Some bottles.\\\"]\\nExample 12:\\nOverall sentence: A room with natural, unfinished, pale wood walls and built in shelves, also has curtained sliding doors that open onto a patio with a lounge chair, a big double bed with a blue blanket, and various framed photos and books.\\nDecomposed sentence: [\\\"A room.\\\"]|[\\\"Natural, unfinished, pale wood walls.\\\"]|[\\\"Built-in shelves.\\\"]|[\\\"Curtained sliding doors.\\\",\\\"Sliding doors that open onto a patio with a lounge chair.\\\"]|[\\\"A big double bed with a blue blanket.\\\"]|[\\\"Framed photos.\\\"]|[\\\"Books.\\\"]\\nExample 13:\\nOverall sentence: A minimally furnished room shows small window pairs with blinds and no curtains, an unmade double bed with a heavy, taupe duvet, and a white border near the ceiling with colorful birds trailing in both directions.\\nDecomposed sentence: [\\\"A minimally furnished room.\\\"]|[\\\"Small window pairs with blinds and no curtains.\\\"]|[\\\"An unmade double bed with a heavy, taupe duvet.\\\"]|[\\\"A white border near the ceiling with colorful birds trailing in both directions.\\\"]\\nExample 14:\\nOverall sentence: A blurry night-time street scene shows a higher and lower roadway with overpasses and lots of traffic going the same way, while to the left an area of red glare with an arrow and number, suggests a caution area.\\nDecomposed sentence: [\\\"A blurry night-time street scene shows a higher and lower roadway with overpasses and lots of traffic going the same way.\\\"]|[\\\"An area of red glare with an arrow and number, suggests a caution area.\\\"]\\nExample 15:\\nOverall sentence: Several people are standing, or sitting, in the vicinity of a scoreboard, to the front of which lies a tennis court, featuring a player with his knees bent, who is also grasping a racquet with both hands,\\nDecomposed sentence: [\\\"Several people are standing, or sitting, in the vicinity of a scoreboard.\\\",\\\"In front of the scoreboard lies a tennis court.\\\"]|[\\\"A tennis player with his knees bent, who is grasping a racquet with both hands.\\\"]\\nExample 16:\\nOverall sentence: Black and white of a blacksmith interior with a horse, a man standing with arm out to it, and a man ducking under it's head and holding a long metal file in both hands, with horses on poles on the wall.\\nDecomposed sentence: [\\\"Black and white photo of a blacksmith interior with a horse.\\\",\\\"Black and white photo of a man standing with arm out to a horse.\\\",\\\"Black and white photo of a man ducking under a horses head and holding a long metal file in both hands.\\\"]|[\\\"Black and white photo with horse poles on the wall.\\\"]\\nExample 17:\\nOverall sentence: A blurry living room scene suggests a wide screen T,V. with a game playing, bookshelves, a low cluttered table, a couch, and closest and clearest, a sports t-shirt with the number thirty-five on it.\\nDecomposed sentence: [\\\"A blurry living room scene.\\\"]|[\\\"A wide screen TV with a sports game playing.\\\"]|[\\\"Bookshelves.\\\"]|[\\\"A low cluttered table.\\\"]|[\\\"A couch.\\\"]|[\\\"A sports t-shirt with the number 35 on it.\\\"]\\nExample 18:\\nOverall sentence: In the first picture a boy is flipping his skateboard, in the second picture is skating and lifting the board slightly off the ground, and in the third he is jumping into the air onto the skateboard.\\nDecomposed sentence: [\\\"A boy is flipping his skateboard.\\\",\\\"A boy is skating and lifting the skateboard slightly off the ground.\\\",\\\"A boy is jumping into the air onto the skateboard.\\\"]\\nExample 19:\\nOverall sentence: A table with a person holding a sandwich in hand over a plate with a knife on it, and another plate with a knife, napkin, and french bread sandwich, along with a bottle and a glass with beverages.\\nDecomposed sentence: [\\\"A table with a person holding a sandwich in hand.\\\",\\\"A sandwich in hand over a plate with a knife on it.\\\"]|[\\\"A plate with a knife, napkin, and french bread sandwich.\\\",\\\"A bottle.\\\",\\\"A glass with beverages.\\\"]\\nExample 20:\\nOverall sentence: An impressive array of motorcycles recedes into the distance between a driveway and a building fronted with cars and a grassy area, as lots of people mill about the driveway, around the bikes.\\nDecomposed sentence: [\\\"An array of motorcycles between a driveway and a building.\\\",\\\"A building fronted with cars and a grassy area.\\\",\\\"Lots of people mill about the driveway, around the bikes.\\\"]\\nIn the provided 20 examples, with overall sentence and their decomposed phrases, dependent phrases are grouped in the same set of brackets, while independent phrases are not grouped under the same set of brackets. Note that we define two sentences as dependent when the two phrases share an explicitly defined physical and material object between them in the overall sentence. Given an overall sentence and two decomposed phrases from user input, output whether the two decomposed phrases are dependent or independent. You may answer in only one digit, 1 meaning dependent, 0 meaning independent. Only output 1 if you are 100% certain. Do not consider any vague dependencies.\"},\n",
        "          {\"role\": \"user\", \"content\": f\"Overall sentence:{sentence}\\n And two decomposed phrases: {segment1}, {segment2}\\nPlease output 1 or 0, 1 meaning they are dependent phrases, 0 meaning they are independent phrases.\"}\n",
        "        ],\n",
        "        'max_tokens': 1,\n",
        "        'n': 1,\n",
        "        'temperature': 0\n",
        "    }\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            async with session.post(\n",
        "                url='https://api.openai.com/v1/chat/completions',\n",
        "                headers={\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {OPENAI_SECRET_KEY}\"},\n",
        "                json=payload,\n",
        "                ssl=ssl.create_default_context(cafile=certifi.where())\n",
        "            ) as response:\n",
        "                response_json = await response.json()\n",
        "            if \"error\" in response_json:\n",
        "                print(f\"OpenAI request failed with error {response_json['error']}\")\n",
        "                if response_json['error']['code'] == 'rate_limit_exceeded':\n",
        "                    retry_after = 10\n",
        "                    await asyncio.sleep(retry_after)  # Wait for the retry-after duration before retrying\n",
        "                continue\n",
        "            result = response_json['choices'][0]['message']['content']\n",
        "            if result in {'1', '0'}:\n",
        "                print(result)\n",
        "                return int(result)\n",
        "            else:\n",
        "                print(\"errors\")\n",
        "                return 0\n",
        "        except Exception as e:\n",
        "            print(f\"Request failed: {e}\")\n",
        "    print(\"errors\")\n",
        "    return 0\n",
        "\n",
        "# Asynchronous function to group dependent segments for a single sentence\n",
        "async def group_dependent_segments_async(session, sentence, segments):\n",
        "    start_time = time.time()  # Start time\n",
        "    segments_list = segments.split('|')\n",
        "    n = len(segments_list)\n",
        "\n",
        "    # Initialize Union-Find\n",
        "    uf = UnionFind(n)\n",
        "\n",
        "    # Perform pairwise comparisons sequentially\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            if uf.find(i) != uf.find(j):  # Check if i and j are already in the same group\n",
        "                result = await check_dependency_async(session, sentence, segments_list[i], segments_list[j])\n",
        "                if result == 1:\n",
        "                    uf.union(i, j)\n",
        "                await asyncio.sleep(2)  # Small delay to avoid hitting limits too quickly\n",
        "            else:\n",
        "              print('already in same group')\n",
        "\n",
        "    # Group segments based on union-find results\n",
        "    groups = {}\n",
        "    for i in range(n):\n",
        "        root = uf.find(i)\n",
        "        if root not in groups:\n",
        "            groups[root] = []\n",
        "        groups[root].append(i)\n",
        "\n",
        "    # Convert indices back to segments\n",
        "    grouped_indices = [\",\".join(map(str, sorted(group))) for group in groups.values()]\n",
        "    end_time = time.time()  # End time\n",
        "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
        "    return \" | \".join(grouped_indices), elapsed_time\n",
        "\n",
        "# Asynchronous function to process multiple sentences in parallel\n",
        "async def process_sentences_in_bulk(sentences_and_segments):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        results = []\n",
        "        for i in range(0, len(sentences_and_segments), BATCH_SIZE):\n",
        "            batch = sentences_and_segments[i:i+BATCH_SIZE]\n",
        "            tasks = [group_dependent_segments_async(session, sentence, segments) for sentence, segments in batch]\n",
        "            batch_results = await asyncio.gather(*tasks)\n",
        "            results.extend(batch_results)\n",
        "            if i + BATCH_SIZE < len(sentences_and_segments):\n",
        "                await asyncio.sleep(10)  # Wait for 60 seconds between batches to respect rate limits\n",
        "    return results\n",
        "\n",
        "def apply_to_dataframe(df, sentence_col, segments_col):\n",
        "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
        "    df_copy = df.copy()\n",
        "    sentences_and_segments = list(zip(df_copy[sentence_col], df_copy[segments_col]))\n",
        "    results = asyncio.run(process_sentences_in_bulk(sentences_and_segments))\n",
        "    df_copy['groups'] = [result[0] for result in results]\n",
        "    df_copy['elapsed_time'] = [result[1] for result in results]\n",
        "    return df_copy\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    begin = time.time()\n",
        "    df = intermediate_df.copy()\n",
        "    # Filter rows that do not start and end with \"\n",
        "    filtered_df = df[~(df['caption_triples_ls'].str.startswith('\"') & df['caption_triples_ls'].str.endswith('\"'))].copy()\n",
        "    # Apply the lambda function to those rows only\n",
        "    filtered_df['caption_triples_ls'] = filtered_df['caption_triples_ls'].apply(lambda x: '\"' + x.split('\"', 1)[-1].rsplit('\"', 1)[0] + '\"')\n",
        "    # Combine the processed rows back with the original DataFrame\n",
        "    result_df = pd.concat([df[(df['caption_triples_ls'].str.startswith('\"') & df['caption_triples_ls'].str.endswith('\"'))], filtered_df]).sort_index()\n",
        "    # Apply function to DataFrame\n",
        "    final_df = apply_to_dataframe(result_df, 'caption_mscoco', 'caption_triples_ls')\n",
        "    end = time.time()\n",
        "    total_time = end - begin\n",
        "    print(f\"Operation took a total of {total_time} seconds.\")\n",
        "    '''\n",
        "    pickle_file_path = '/content/drive/MyDrive/mscoco_500decomposed_dependencies_v2.pkl'\n",
        "    with open(pickle_file_path, 'wb') as f:\n",
        "      pickle.dump(final_df, f)\n",
        "    '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPtyF5_nmjgw",
        "outputId": "662149f0-d8d1-48b7-88c7-621133484f84"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "already in same group\n",
            "already in same group\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "already in same group\n",
            "0\n",
            "already in same group\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "1\n",
            "0\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "0\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "already in same group\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "already in same group\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "Operation took a total of 96.3318862915039 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "id": "WJOcamJtITMt",
        "outputId": "977a5b06-3bb2-43b9-e229-2b17c3cc6590"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id             image  \\\n",
              "0  553992  000000553992.jpg   \n",
              "1   39270  000000039270.jpg   \n",
              "2  277161  000000277161.jpg   \n",
              "3  380070  000000380070.jpg   \n",
              "4  357272  000000357272.jpg   \n",
              "5  480770  000000480770.jpg   \n",
              "6  515241  000000515241.jpg   \n",
              "7  560604  000000560604.jpg   \n",
              "8  158005  000000158005.jpg   \n",
              "9  449712  000000449712.jpg   \n",
              "\n",
              "                                  caption_sharegpt4v  \\\n",
              "0  In the image, a man and a woman are standing i...   \n",
              "1  In the image, there is a man who is in the mid...   \n",
              "2  In the center of the image, a vintage Apple II...   \n",
              "3  The image captures a cozy hotel room, bathed i...   \n",
              "4  The image captures a room that exudes a vintag...   \n",
              "5  In the heart of a bustling city, an elderly wo...   \n",
              "6  The image captures a dynamic moment in a baseb...   \n",
              "7  In the center of the image, a black stuffed an...   \n",
              "8  In the dimly lit ambiance of the room, a dinne...   \n",
              "9  In the image, a group of people are seen walki...   \n",
              "\n",
              "                                      caption_mscoco  \\\n",
              "0  An institutional looking room holds an exit si...   \n",
              "1  Black night, a blurry shot shows a tall fence ...   \n",
              "2  An older looking model computer components; tw...   \n",
              "3  A motel-like room  features all of its furnitu...   \n",
              "4  A room shows a twin bed, crib, double bed, and...   \n",
              "5  A street view shows a building entrance and tr...   \n",
              "6  Off in the distance, a few people dot the stan...   \n",
              "7  A counter top with a plate with a fork and few...   \n",
              "8  Indirect lighting in a darkened room plays acr...   \n",
              "9  the long view of a parking lot shows a row of ...   \n",
              "\n",
              "                                           embedding  \\\n",
              "0  [0.25389287, 0.2295701, -0.108173296, -0.07395...   \n",
              "1  [0.044150963, 0.17922473, -0.14461896, -0.1534...   \n",
              "2  [-0.41152698, -0.29223415, 0.025055759, -0.164...   \n",
              "3  [0.23270163, 0.07637343, 0.36209187, -0.012398...   \n",
              "4  [0.45787933, 0.13048139, 0.11703592, 0.0351443...   \n",
              "5  [0.20354465, -0.0718597, -0.27712327, -0.49626...   \n",
              "6  [-0.029054046, 0.23667967, 0.111569144, 0.0622...   \n",
              "7  [-0.29823345, 0.32871175, -0.07593347, -0.1097...   \n",
              "8  [-0.18686089, 0.35955262, 0.008806132, -0.3130...   \n",
              "9  [0.19945762, -0.1157327, -0.03143066, 0.023830...   \n",
              "\n",
              "   original_mscoco_caption_length  \\\n",
              "0                             249   \n",
              "1                             246   \n",
              "2                             244   \n",
              "3                             243   \n",
              "4                             242   \n",
              "5                             241   \n",
              "6                             235   \n",
              "7                             230   \n",
              "8                             229   \n",
              "9                             226   \n",
              "\n",
              "                                  caption_triples_ls  \\\n",
              "0  \"An exit sign.\"|\"Chairs.\"|\"Recording equipment...   \n",
              "1  \"In a black night, a blurry shot shows a tall ...   \n",
              "2  \"An older looking model computer components.\"|...   \n",
              "3  \"A motel-like room.\"|\"All of its furniture in ...   \n",
              "4  \"A room.\"|\"A twin bed featuring checks and pil...   \n",
              "5  \"A street view shows a building entrance.\"|\"A ...   \n",
              "6  \"A few people dot the stands bordering a ball ...   \n",
              "7  \"A counter top.\"|\"A plate with a fork and a fe...   \n",
              "8  \"Indirect lighting in a darkened room shines a...   \n",
              "9  \"A parking lot shows a row of parked vehicles....   \n",
              "\n",
              "                         groups  elapsed_time  \n",
              "0               0 | 1 | 2 | 3,4     29.285209  \n",
              "1                         0,1,2      6.904167  \n",
              "2                   0,1,3,4 | 2     24.884712  \n",
              "3  0,1,2,3,4,5,6,7,8,9,10,11,12     36.476129  \n",
              "4   0,1,3,4 | 2 | 5 | 6 | 7 | 8     96.079601  \n",
              "5                   0,1 | 2,3,4     28.337717  \n",
              "6                 0,1 | 2,3,4,5     35.038784  \n",
              "7           0,1,2 | 3 | 4 | 5,6     56.153500  \n",
              "8                   0,1,2,3,4,5     14.470415  \n",
              "9                         0,1,2      6.133904  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be322b9e-edeb-4fdb-9288-8c72ef77a235\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>image</th>\n",
              "      <th>caption_sharegpt4v</th>\n",
              "      <th>caption_mscoco</th>\n",
              "      <th>embedding</th>\n",
              "      <th>original_mscoco_caption_length</th>\n",
              "      <th>caption_triples_ls</th>\n",
              "      <th>groups</th>\n",
              "      <th>elapsed_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>553992</td>\n",
              "      <td>000000553992.jpg</td>\n",
              "      <td>In the image, a man and a woman are standing i...</td>\n",
              "      <td>An institutional looking room holds an exit si...</td>\n",
              "      <td>[0.25389287, 0.2295701, -0.108173296, -0.07395...</td>\n",
              "      <td>249</td>\n",
              "      <td>\"An exit sign.\"|\"Chairs.\"|\"Recording equipment...</td>\n",
              "      <td>0 | 1 | 2 | 3,4</td>\n",
              "      <td>29.285209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39270</td>\n",
              "      <td>000000039270.jpg</td>\n",
              "      <td>In the image, there is a man who is in the mid...</td>\n",
              "      <td>Black night, a blurry shot shows a tall fence ...</td>\n",
              "      <td>[0.044150963, 0.17922473, -0.14461896, -0.1534...</td>\n",
              "      <td>246</td>\n",
              "      <td>\"In a black night, a blurry shot shows a tall ...</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>6.904167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>277161</td>\n",
              "      <td>000000277161.jpg</td>\n",
              "      <td>In the center of the image, a vintage Apple II...</td>\n",
              "      <td>An older looking model computer components; tw...</td>\n",
              "      <td>[-0.41152698, -0.29223415, 0.025055759, -0.164...</td>\n",
              "      <td>244</td>\n",
              "      <td>\"An older looking model computer components.\"|...</td>\n",
              "      <td>0,1,3,4 | 2</td>\n",
              "      <td>24.884712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>380070</td>\n",
              "      <td>000000380070.jpg</td>\n",
              "      <td>The image captures a cozy hotel room, bathed i...</td>\n",
              "      <td>A motel-like room  features all of its furnitu...</td>\n",
              "      <td>[0.23270163, 0.07637343, 0.36209187, -0.012398...</td>\n",
              "      <td>243</td>\n",
              "      <td>\"A motel-like room.\"|\"All of its furniture in ...</td>\n",
              "      <td>0,1,2,3,4,5,6,7,8,9,10,11,12</td>\n",
              "      <td>36.476129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>357272</td>\n",
              "      <td>000000357272.jpg</td>\n",
              "      <td>The image captures a room that exudes a vintag...</td>\n",
              "      <td>A room shows a twin bed, crib, double bed, and...</td>\n",
              "      <td>[0.45787933, 0.13048139, 0.11703592, 0.0351443...</td>\n",
              "      <td>242</td>\n",
              "      <td>\"A room.\"|\"A twin bed featuring checks and pil...</td>\n",
              "      <td>0,1,3,4 | 2 | 5 | 6 | 7 | 8</td>\n",
              "      <td>96.079601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>480770</td>\n",
              "      <td>000000480770.jpg</td>\n",
              "      <td>In the heart of a bustling city, an elderly wo...</td>\n",
              "      <td>A street view shows a building entrance and tr...</td>\n",
              "      <td>[0.20354465, -0.0718597, -0.27712327, -0.49626...</td>\n",
              "      <td>241</td>\n",
              "      <td>\"A street view shows a building entrance.\"|\"A ...</td>\n",
              "      <td>0,1 | 2,3,4</td>\n",
              "      <td>28.337717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>515241</td>\n",
              "      <td>000000515241.jpg</td>\n",
              "      <td>The image captures a dynamic moment in a baseb...</td>\n",
              "      <td>Off in the distance, a few people dot the stan...</td>\n",
              "      <td>[-0.029054046, 0.23667967, 0.111569144, 0.0622...</td>\n",
              "      <td>235</td>\n",
              "      <td>\"A few people dot the stands bordering a ball ...</td>\n",
              "      <td>0,1 | 2,3,4,5</td>\n",
              "      <td>35.038784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>560604</td>\n",
              "      <td>000000560604.jpg</td>\n",
              "      <td>In the center of the image, a black stuffed an...</td>\n",
              "      <td>A counter top with a plate with a fork and few...</td>\n",
              "      <td>[-0.29823345, 0.32871175, -0.07593347, -0.1097...</td>\n",
              "      <td>230</td>\n",
              "      <td>\"A counter top.\"|\"A plate with a fork and a fe...</td>\n",
              "      <td>0,1,2 | 3 | 4 | 5,6</td>\n",
              "      <td>56.153500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>158005</td>\n",
              "      <td>000000158005.jpg</td>\n",
              "      <td>In the dimly lit ambiance of the room, a dinne...</td>\n",
              "      <td>Indirect lighting in a darkened room plays acr...</td>\n",
              "      <td>[-0.18686089, 0.35955262, 0.008806132, -0.3130...</td>\n",
              "      <td>229</td>\n",
              "      <td>\"Indirect lighting in a darkened room shines a...</td>\n",
              "      <td>0,1,2,3,4,5</td>\n",
              "      <td>14.470415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>449712</td>\n",
              "      <td>000000449712.jpg</td>\n",
              "      <td>In the image, a group of people are seen walki...</td>\n",
              "      <td>the long view of a parking lot shows a row of ...</td>\n",
              "      <td>[0.19945762, -0.1157327, -0.03143066, 0.023830...</td>\n",
              "      <td>226</td>\n",
              "      <td>\"A parking lot shows a row of parked vehicles....</td>\n",
              "      <td>0,1,2</td>\n",
              "      <td>6.133904</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be322b9e-edeb-4fdb-9288-8c72ef77a235')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-be322b9e-edeb-4fdb-9288-8c72ef77a235 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-be322b9e-edeb-4fdb-9288-8c72ef77a235');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-290108f7-71cf-4abd-92d9-61d0a2263963\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-290108f7-71cf-4abd-92d9-61d0a2263963')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-290108f7-71cf-4abd-92d9-61d0a2263963 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_daef7e05-348f-47cb-8c76-648c4262cc6f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('final_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_daef7e05-348f-47cb-8c76-648c4262cc6f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('final_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "final_df",
              "summary": "{\n  \"name\": \"final_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 174033,\n        \"min\": 39270,\n        \"max\": 560604,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          158005,\n          39270,\n          480770\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"000000158005.jpg\",\n          \"000000039270.jpg\",\n          \"000000480770.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_sharegpt4v\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"In the dimly lit ambiance of the room, a dinner table takes center stage, draped in a vibrant red tablecloth. The centerpiece of the table is a plate filled with an assortment of colorful dishes, their hues muted under the soft lighting. To the left and right of the plate are two glasses filled with water, their surfaces gleaming subtly in the light. \\n\\nOn the left side of the table, a basket filled with bread adds a rustic touch to the setting. The background fades into a dark and blurry abyss, drawing focus to the meal laid out on the table. The entire scene paints a picture of a quiet, intimate dinner, waiting to be savored.\",\n          \"In the image, there is a man who is in the midst of a powerful swing at a batting cage. He is dressed in a black shirt and is wearing a red helmet, indicating that he is prepared for the sport. The man's stance and the blur of the bat suggest that he is in action, possibly hitting a baseball.\\n\\nThe image captures him from behind and slightly to the right, providing a clear view of his swing and posture. The batting cage, surrounded by a chain link fence, is the main setting of this scene. Despite the darkness of the surroundings, one can make out faint lights and buildings in the distance, suggesting that this could be an urban setting or a sports complex.\\n\\nThe focus of the image is clearly on the batter and his action, with the background elements serving to provide context to the scene. The image does not contain any text or other discernible objects. The overall atmosphere suggests an intense practice session or perhaps even a late-night game.\",\n          \"In the heart of a bustling city, an elderly woman finds solace on a wooden bench. She's dressed in a beige jacket and blue jeans, her eyes shielded from the sun by a pair of sunglasses. The bench, her temporary haven, is surrounded by a group of pigeons, their feathers a mix of gray, black, and white. They peck at the ground, perhaps in search of food or simply enjoying the day.\\n\\nBehind the bench, a black fence stands guard, separating the woman from the world beyond. A blue car is parked on the street, its vibrant color contrasting with the muted tones of the cityscape. In the distance, a building with a red awning adds a splash of color to the scene. The woman, the pigeons, the car, and the building - each element tells a story of city life.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_mscoco\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Indirect lighting in a darkened room plays across a table covered with the remains of a meal, thereby giving everything, including the table, a basket, plates with uneaten food, utensils and glasses with liquid, a reddish patina.\",\n          \"Black night, a blurry shot shows a tall fence that angles, in a kind of Z shape, around what appears to be  a parking area  with a large building fronting it, as a kid with a red helmet takes a fierce swing in the batting area, inside the fence. \",\n          \"A street view shows a building entrance and traffic, including a bus, in the background, and to the foreground, a black iron fence, and a  bench with an older, seated person, looking at a flock of pigeons scattered on the ground and bench.  \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"original_mscoco_caption_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 226,\n        \"max\": 249,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          229,\n          246,\n          241\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"caption_triples_ls\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"\\\"Indirect lighting in a darkened room shines across a table covered with the remains of a meal.\\\"|\\\"The table has a reddish patina.\\\"|\\\"The basket has a reddish patina.\\\"|\\\"The plates with uneaten food have a reddish patina.\\\"|\\\"Utensils have a reddish patina.\\\"|\\\"Glasses with liquid have a reddish patina.\\\"\",\n          \"\\\"In a black night, a blurry shot shows a tall fence that angles, in a kind of Z shape.\\\"|\\\"Around appears to be a parking area with a large building fronting it.\\\"|\\\"A kid with a red helmet takes a fierce swing in the batting area, inside the fence.\\\"\",\n          \"\\\"A street view shows a building entrance.\\\"|\\\"A street view shows traffic, including a bus in the background.\\\"|\\\"In the foreground is a black iron fence.\\\"|\\\"In the foreground is a bench with an older person sitting on it.\\\"|\\\"A flock of pigeons scattered on the ground and bench.\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"groups\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"0,1,2 | 3 | 4 | 5,6\",\n          \"0,1,2\",\n          \"0,1 | 2,3,4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"elapsed_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 26.619674125567503,\n        \"min\": 6.133903741836548,\n        \"max\": 96.07960081100464,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          14.470415115356445,\n          6.9041666984558105,\n          28.337717056274414\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CadcfGAjKumB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}